{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50b410b",
   "metadata": {},
   "source": [
    "# ResNet TOV Emulator to predict Mass, Radius and Tidal Deformability of a Neutron Strar from a Tabular (Hybrid) EOS - Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ac8f7",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Import all the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08120107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418ae44",
   "metadata": {},
   "source": [
    "### <div style= 'color: Slateblue'> Create dataset of input and oputput samples from the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. Dataset Configuration\n",
    "# ==============================================================================\n",
    "NUM_FILES_TO_USE = 100 # Increased for better generalization\n",
    "DATA_DIR_PATH = \"/home/kay/ML-for-Neutron-Stars/All_MR_Relations/All_MR_Relations/\"\n",
    "save_dir = f\"{NUM_FILES_TO_USE}files\"\n",
    "\n",
    "# Regex to extract parameters\n",
    "pattern = re.compile(\n",
    "    r\"MREoSm(?P<m>\\d+)\"\n",
    "    r\"L(?P<L>\\d+)\"\n",
    "    r\"J(?P<J>\\d+)\"\n",
    "    r\"T(?P<T>\\d+)_\"\n",
    "    r\"n(?P<n>\\d+)_\"\n",
    "    r\"Yp?\\+.*?RGgen_\"\n",
    "    r\"v(?P<n_v>[-\\d\\.]+)\"\n",
    "    r\"d(?P<d>[-\\d\\.]+)\"\n",
    "    r\"B(?P<Btype>[np])(?P<B>\\d+)\\.dat\"\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Processing files\n",
    "# ==============================================================================\n",
    "all_files = glob.glob(os.path.join(DATA_DIR_PATH, \"MR*.dat\"))\n",
    "if not all_files: raise RuntimeError(\"No files found.\")\n",
    "\n",
    "# Shufflling files\n",
    "np.random.shuffle(all_files)\n",
    "selected_files = all_files[:NUM_FILES_TO_USE]\n",
    "\n",
    "# Split files into Train and Validation lists (80 / 20)\n",
    "split_idx = int(0.8 * len(selected_files))\n",
    "train_files = selected_files[:split_idx]\n",
    "val_files = selected_files[split_idx:]\n",
    "\n",
    "def process_files(file_list, label_desc):\n",
    "    dataset_rows = []\n",
    "    files_used_for_training = [] # Save files used for training so we can ommit\n",
    "                                 # these later on when testing the model\n",
    "    \n",
    "    for file_path in tqdm(file_list, desc=f\"Processing {label_desc}\"):\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if not match: continue\n",
    "\n",
    "        files_used_for_training.append(file_path)\n",
    "\n",
    "        # 1. Extract All input parameters\n",
    "        m = float(match.group(\"m\")) / 100.0 \n",
    "        L = float(match.group(\"L\"))\n",
    "        J = float(match.group(\"J\"))             \n",
    "        T = float(match.group(\"T\")) # Unused but extracted for completeness             \n",
    "        n = float(match.group('n')) / 1000.0\n",
    "        n_v = float(match.group(\"n_v\"))\n",
    "        d = float(match.group(\"d\"))\n",
    "        B = float(match.group(\"B\")) / 1000.0\n",
    "        \n",
    "        # 2. Load the injection and output parameters from each file\n",
    "        try:\n",
    "            data = np.loadtxt(file_path)\n",
    "        except: continue\n",
    "\n",
    "        if data.ndim == 1 or data.shape[1] < 3: continue\n",
    "\n",
    "        # Columns: [Central pressure (P_c), Mass, Radius, Tidal Deformability,\n",
    "        # R_quarkcore, Baryonic mass (rho)]\n",
    "        cp = data[:, 0]\n",
    "        mass = data[:, 1]\n",
    "        radius = data[:, 2]\n",
    "        td = data[:, 3]\n",
    "\n",
    "        # 3. Filter out Unstable Branch\n",
    "        # We only want data up to the Maximum Mass. \n",
    "        # Usually Data is sorted by central pressure.\n",
    "        # M increases then decreases.\n",
    "        max_m_index = np.argmax(mass)\n",
    "        \n",
    "        # Truncate BEFORE the noisy peak at M_max\n",
    "        # Drop the last 2 points from the calculated stable branch\n",
    "        SAFETY_MARGIN = 2 \n",
    "        \n",
    "        # Ensure we don't end up with negative index\n",
    "        end_index = max(1, max_m_index + 1 - SAFETY_MARGIN)\n",
    "        \n",
    "        # Slice arrays to only keep stable, safe branch\n",
    "        cp = cp[:end_index]\n",
    "        mass = mass[:end_index] \n",
    "        radius = radius[:end_index]\n",
    "        td = td[:end_index]\n",
    "\n",
    "        # 4. Apply Minimum Mass Cutoff (Filter low-mass instability)\n",
    "        M_CUTOFF = 0.15 # Filter out anything below 0.15 solar masses\n",
    "        low_mass_mask = mass >= M_CUTOFF\n",
    "        cp= cp[low_mass_mask]\n",
    "        mass = mass[low_mass_mask]\n",
    "        radius = radius[low_mass_mask]\n",
    "        td = td[low_mass_mask]\n",
    "\n",
    "        # 5. Basic filtering to ensure positive values\n",
    "        valid_mask = (radius > 0) & (mass > 0)\n",
    "        cp = cp[valid_mask]\n",
    "        mass = mass[valid_mask]\n",
    "        radius = radius[valid_mask]\n",
    "        td = td[valid_mask]\n",
    "\n",
    "        if len(mass) == 0: continue\n",
    "\n",
    "        # 6. Create feature vector for every point\n",
    "        # Features: [m, L, J, n_v, d, B, n, cp] -> Predict: Mass, Radius, TD\n",
    "        num_points = len(mass)\n",
    "        eos_params = np.array([m, L, J, n_v, d, B, n])\n",
    "        \n",
    "        # 7. Tile EOS params to match number of mass points\n",
    "        eos_repeated = np.tile(eos_params, (num_points, 1))\n",
    "\n",
    "        # 8. Stack: [EOS_Params (7), Central Pressure, Mass, Radius, TD]\n",
    "        rows = np.column_stack([eos_repeated, cp, mass, radius, td])\n",
    "        dataset_rows.append(rows)\n",
    "\n",
    "    if not dataset_rows: \n",
    "        return None\n",
    "    \n",
    "    return np.vstack(dataset_rows), np.array(files_used_for_training)\n",
    "\n",
    "print(\"Building Training Set...\")\n",
    "train_data = process_files(train_files, \"Train\")\n",
    "print(\"Building Validation Set...\")\n",
    "val_data = process_files(val_files, \"Val\")\n",
    "print(\"Datasets created!\")\n",
    "\n",
    "print(train_data[1].shape)\n",
    "print(val_data[1].shape)\n",
    "\n",
    "# Save files used for training and validation to be used during testing of model\n",
    "files_used_for_training = np.concatenate([train_data[1], val_data[1]]) if train_data and val_data is not None else []\n",
    "\n",
    "# Save all datasets\n",
    "os.makedirs(save_dir, exist_ok=True)    # Create folder if it doesn't exist\n",
    "\n",
    "np.save(os.path.join(save_dir, \"train_data_files.npy\"), train_data[0])\n",
    "np.save(os.path.join(save_dir, \"val_data_files.npy\"), val_data[0])\n",
    "np.save(os.path.join(save_dir, \"files_used_for_training.npy\"), files_used_for_training)\n",
    "\n",
    "print(\"Datasets saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05456348",
   "metadata": {},
   "source": [
    "# <div style= 'color: red'> FROM HERE ON USE CLUSTER FILE train_real.py TO TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53906f3",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> PyTorch ML pipeline - loading datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this \"save_dir\" only when loding a saved model and datasets\n",
    "# save_dir = \"1000files/\"\n",
    "\n",
    "train_data = np.load(os.path.join(save_dir, \"train_data_files.npy\"))\n",
    "val_data = np.load(os.path.join(save_dir, \"val_data_files.npy\"))\n",
    "files_used_for_training = np.load(os.path.join(save_dir, \"files_used_for_training.npy\"))\n",
    "train_data.shape, val_data.shape, files_used_for_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a72fd",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Range of values of the parameters from the dataset <b>before</b> scaling and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nuclear Mass min/max:\", np.round(train_data[:, 0].min(), 3), np.round(train_data[:, 0].max(), 3))\n",
    "print(\"Slope parmeter min/max:\", np.round(train_data[:, 1].min(), 3), np.round(train_data[:, 1].max(), 3))\n",
    "print(\"Symmetry Energy min/max:\", np.round(train_data[:, 2].min(), 3), np.round(train_data[:, 2].max(), 3))\n",
    "print(\"Vector coupling min/max:\", np.round(train_data[:, 3].min(), 3), np.round(train_data[:, 3].max(), 3))\n",
    "print(\"Diquark coupling min/max:\", np.round(train_data[:, 4].min(), 3), np.round(train_data[:, 4].max(), 3))\n",
    "print(\"Bag constant min/max:\", np.round(train_data[:, 5].min(), 3), np.round(train_data[:, 5].max(), 3))\n",
    "print(\"Number of particles min/max:\", np.round(train_data[:, 6].min(), 3), np.round(train_data[:, 6].max(), 3))\n",
    "print(\"Central Pressure min/max:\", np.round(train_data[:, 7].min(), 3), np.round(train_data[:, 7].max(), 3))\n",
    "print(\"\")\n",
    "print(\"Mass min/max:\", np.round(train_data[:, 8].min(), 3), np.round(train_data[:, 8].max(), 3))\n",
    "print(\"Radius min/max:\", np.round(train_data[:, 9].min(), 3), np.round(train_data[:, 9].max(), 3))\n",
    "print(\"Tidal Deformability min/max:\", np.round(train_data[:, 10].min(), 3), np.round(train_data[:, 10].max(), 3))\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c2967",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Produce a mass-radius curve for a single EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f701c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================================\n",
    "# Extract ONE eos curve\n",
    "# ==========================================================\n",
    "# Pick a specific index (0 = first file in train_data before shuffling)\n",
    "# But because the train_data is shuffled, we instead extract a *single EOS*\n",
    "# by matching identical (m, L, J, n_v, d, B, n).\n",
    "# Selecting the EOS at index 20000 for variety\n",
    "\n",
    "# Getting the input parameters\n",
    "m0, L0, J0, nv0, d0, B0, n0 = train_data[15000, :7]\n",
    "mask = (\n",
    "    (train_data[:,0] == m0)  &\n",
    "    (train_data[:,1] == L0)  &\n",
    "    (train_data[:,2] == J0)  &\n",
    "    (train_data[:,3] == nv0) &\n",
    "    (train_data[:,4] == d0)  &\n",
    "    (train_data[:,5] == B0)  &\n",
    "    (train_data[:,6] == n0)\n",
    ")\n",
    "subset = train_data[mask]\n",
    "\n",
    "# Getting the outut paramters\n",
    "mass = subset[:, 8]\n",
    "radius = subset[:, 9]\n",
    "\n",
    "# Sort by mass so plot looks clean\n",
    "order = np.argsort(mass)\n",
    "mass = mass[order]\n",
    "radius = radius[order]\n",
    "\n",
    "# ==========================================================\n",
    "# Plot\n",
    "# ==========================================================\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(radius, mass, linewidth=2)\n",
    "plt.xlabel(\"Radius [km]\")\n",
    "plt.ylabel(\"Mass [M☉]\")\n",
    "plt.title(f\"EOS sanity check\\nm={m0}, L={L0}, J={J0}, n_v={nv0}, d={d0}, B={B0}, n={n0}\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73ed77",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Produce a TD-compactness curve for a single EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================================\n",
    "# Extract ONE eos curve\n",
    "# ==========================================================\n",
    "# Pick a specific index (0 = first file in train_data before shuffling)\n",
    "# But because the train_data is shuffled, we instead extract a *single EOS*\n",
    "# by matching identical (m, L, J, n_v, d, B, n).\n",
    "# Selecting the EOS at index 20000 for variety\n",
    "\n",
    "# Getting the input parameters\n",
    "m0, L0, J0, nv0, d0, B0, n0 = train_data[15000, :7]\n",
    "mask = (\n",
    "    (train_data[:,0] == m0)  &\n",
    "    (train_data[:,1] == L0)  &\n",
    "    (train_data[:,2] == J0)  &\n",
    "    (train_data[:,3] == nv0) &\n",
    "    (train_data[:,4] == d0)  &\n",
    "    (train_data[:,5] == B0)  &\n",
    "    (train_data[:,6] == n0)\n",
    ")\n",
    "subset = train_data[mask]\n",
    "\n",
    "# Getting the output parameters\n",
    "mass = subset[:, 8]\n",
    "radius = subset[:, 9]\n",
    "td = np.log10(subset [:, 10])\n",
    "\n",
    "# Sort by mass so plot looks clean\n",
    "order = np.argsort(mass)\n",
    "mass = mass[order]\n",
    "radius = radius[order]\n",
    "td = td[order]\n",
    "\n",
    "# Calculate compactness\n",
    "compact = mass / radius\n",
    "\n",
    "# ==========================================================\n",
    "# Plot\n",
    "# ==========================================================\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(compact, td, linewidth=2)\n",
    "plt.xlabel(\"Compactness (M/R)\")\n",
    "plt.ylabel(\"Tidal Deformability (log_10)\")\n",
    "plt.title(f\"EOS sanity check\\nm={m0}, L={L0}, J={J0}, n_v={nv0}, d={d0}, B={B0}, n={n0}\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892617e5",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'>PyTorch ML pipeline - prepare data\n",
    "\n",
    "* Z_Score normalization: $X_{\\text{norm}, i} = (X_i - \\mu_i) / \\sigma_i$\n",
    "* Constant Scaling: $X_{\\text{norm}, M} = X_M / 3.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and batch size for training \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 256 \n",
    "\n",
    "# ==========================================================\n",
    "# 1. Splitting into training and validation sets was performed at the file level\n",
    "# ==========================================================\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Convert to Tensor\n",
    "# ==========================================================\n",
    "# Convert to Tensor\n",
    "# Inputs: Cols 0 to 7 (8 features: m, L, J, n_v, d, B, n, cp)\n",
    "# Output: Col 8, 9 and 10 (Mass, Radius, Tidal Deformability)\n",
    "X_train = torch.tensor(train_data[:, :8], dtype=torch.float32)  # Training Input\n",
    "y_train = torch.tensor(train_data[:, 8:], dtype=torch.float32)  # Training Output\n",
    "\n",
    "X_val = torch.tensor(val_data[:, :8], dtype=torch.float32)      # Validation Input\n",
    "y_val = torch.tensor(val_data[:, 8:], dtype=torch.float32)      # Validation Output\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Normalize the inputs\n",
    "# ==========================================================\n",
    "# 3.1. Separate Central Pressure (index 7) from EOS parameters (indices 0-7)\n",
    "X_eos_train, X_cp_train = X_train[:, :7], X_train[:, 7:]\n",
    "X_eos_val, X_cp_val = X_val[:, :7], X_val[:, 7:]\n",
    "\n",
    "# 3.2. Log Scaling on Central Pressure (cp)\n",
    "X_cp_train_norm = torch.log10(X_cp_train)\n",
    "X_cp_val_norm = torch.log10(X_cp_val)\n",
    "\n",
    "# 3.3 Normalize using Z-Score on EOS Parameters (m, L, J, n_v, d, B, n)\n",
    "X_eos_mean = X_eos_train.mean(dim=0, keepdim=True)\n",
    "X_eos_std = X_eos_train.std(dim=0, keepdim=True)\n",
    "X_eos_std[X_eos_std == 0] = 1.0 \n",
    "# Save the normalization statistics to be used later for model testing\n",
    "torch.save(X_eos_mean, os.path.join(save_dir, \"X_eos_mean.pt\"))\n",
    "torch.save(X_eos_std, os.path.join(save_dir, \"X_eos_std.pt\"))\n",
    "print(\"Normalization statistics saved.\")\n",
    "X_eos_train_norm = (X_eos_train - X_eos_mean) / X_eos_std\n",
    "X_eos_val_norm = (X_eos_val - X_eos_mean) / X_eos_std\n",
    "\n",
    "# 3.4. Recombine Inputs\n",
    "X_train_norm = torch.cat((X_eos_train_norm, X_cp_train_norm), dim=1)\n",
    "X_val_norm = torch.cat((X_eos_val_norm, X_cp_val_norm), dim=1)\n",
    "\n",
    "# ==========================================================\n",
    "# 4. Normalize the outputs\n",
    "# ==========================================================\n",
    "# 4.1. Separate Mass, Radius and Tidal Deformability\n",
    "y_mass_train, y_radius_train, y_td_train = y_train[:, 0:1], y_train[:, 1:2], y_train[:, 2:3]\n",
    "y_mass_val, y_radius_val, y_td_val = y_val[:, 0:1], y_val[:, 1:2], y_val[:, 2:3]\n",
    "\n",
    "# 4.2. Constant Scaling on Mass (M)\n",
    "MASS_SCALE = 3.5 \n",
    "y_mass_train_norm = y_mass_train / MASS_SCALE\n",
    "y_mass_val_norm = y_mass_val / MASS_SCALE\n",
    "\n",
    "# 4.3. Constant Scaling on Radius (R)\n",
    "RADIUS_SCALE = 25.0\n",
    "y_radius_train_norm = y_radius_train / RADIUS_SCALE\n",
    "y_radius_val_norm = y_radius_val / RADIUS_SCALE\n",
    "\n",
    "# 4.4. Log scaling on Tidal Deformability (td)\n",
    "y_td_train_norm = torch.log10(y_td_train)\n",
    "y_td_val_norm = torch.log10(y_td_val)\n",
    "\n",
    "# 4.5. Recombine Outputs\n",
    "y_train_norm = torch.cat((y_mass_train_norm, y_radius_train_norm, y_td_train_norm), dim=1)\n",
    "y_val_norm = torch.cat((y_mass_val_norm, y_radius_val_norm, y_td_val_norm), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cb7d0",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Range of values of the parameters after scaling and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nuclear Mass min/max:\", np.round(X_train_norm[:, 0].min(), 3), np.round(X_train_norm[:, 0].max(), 3))\n",
    "print(\"Slope parmeter min/max:\", np.round(X_train_norm[:, 1].min(), 3), np.round(X_train_norm[:, 1].max(), 3))\n",
    "print(\"Symmetry Energy min/max:\", np.round(X_train_norm[:, 2].min(), 3), np.round(X_train_norm[:, 2].max(), 3))\n",
    "print(\"Vector coupling min/max:\", np.round(X_train_norm[:, 3].min(), 3), np.round(X_train_norm[:, 3].max(), 3))\n",
    "print(\"Diquark coupling min/max:\", np.round(X_train_norm[:, 4].min(), 3), np.round(X_train_norm[:, 4].max(), 3))\n",
    "print(\"Bag constant min/max:\", np.round(X_train_norm[:, 5].min(), 3), np.round(X_train_norm[:, 5].max(), 3))\n",
    "print(\"Number of particles min/max:\", np.round(X_train_norm[:, 6].min(), 3), np.round(X_train_norm[:, 6].max(), 3))\n",
    "print(\"Central Pressure min/max:\", np.round(X_train_norm[:, 7].min(), 3), np.round(X_train_norm[:, 7].max(), 3))\n",
    "print(\"\")\n",
    "print(\"Mass min/max:\", np.round(y_train_norm[:, 0].min(), 3), np.round(y_train_norm[:, 0].max(), 3))\n",
    "print(\"Radius min/max:\", np.round(y_train_norm[:, 1].min(), 3), np.round(y_train_norm[:, 1].max(), 3))\n",
    "print(\"Tidal Deformability min/max:\", np.round(y_train_norm[:, 2].min(), 3), np.round(y_train_norm[:, 2].max(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd70e28",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'>Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Single Residual Network Block\n",
    "# ==========================================================\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, auxiliary_dim=1):\n",
    "        super().__init__()\n",
    "        # We accept the hidden state + the auxiliary Central Pressure injection\n",
    "        self.fc = nn.Linear(hidden_dim + auxiliary_dim, hidden_dim)\n",
    "        self.act = nn.GELU() # Gaussian Error Linear Unit\n",
    "    \n",
    "    def forward(self, x, cp):\n",
    "        # Concatenate Central Pressure to the input of the layer\n",
    "        combined = torch.cat([x, cp], dim=1)\n",
    "        out = self.act(self.fc(combined))\n",
    "        return x + out # Residual connection\n",
    "\n",
    "# ==========================================================\n",
    "# Set up the Residual Network flow\n",
    "# ==========================================================\n",
    "class PhysicsEmulator(nn.Module):\n",
    "    def __init__(self, input_dim=8, hidden_dim=512): \n",
    "        super().__init__()\n",
    "        # Separate EOS inputs from Central Pressure\n",
    "        # input_dim is 8: (7 EOS params + 1 Central Pressure)\n",
    "        self.eos_dim = input_dim - 1 \n",
    "        \n",
    "        # 1. Initial encoding of EOS parameters only: \n",
    "        self.input_layer = nn.Linear(self.eos_dim, hidden_dim)\n",
    "        \n",
    "        # 2. Deep Residual Layers with Central Pressure Injection\n",
    "        self.block1 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "        self.block2 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "        self.block3 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "        self.block4 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "\n",
    "        # 3. Output layers\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 1, hidden_dim // 2), # Inject Central Pressure one last time\n",
    "            nn.GELU(),                                      # 513 --> 256\n",
    "            nn.Linear(hidden_dim // 2, 3)                   # Output: 256 --> 3\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into EOS params and Central Pressure\n",
    "        # Central Pressure is the LAST column (index -1)\n",
    "        eos_params = x[:, :-1]\n",
    "        cp = x[:, -1:]\n",
    "        \n",
    "        # 1. Encode EOS\n",
    "        x_hidden = self.input_layer(eos_params)           # Input: 7 --> 512\n",
    "\n",
    "        # 2. Central Pressure-Injected Residual Flow: Pass through blocks, injecting Central Pressure at each step\n",
    "        x_hidden = self.block1(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        x_hidden = self.block2(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        x_hidden = self.block3(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        x_hidden = self.block4(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        \n",
    "        # 3. Final Prediction\n",
    "        # Concatenate Central Pressure one last time for the read-out\n",
    "        combined_final = torch.cat([x_hidden, cp], dim=1) # 512 + 1 = 513\n",
    "        return self.final_layer(combined_final)             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50670815",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Train the model (batch-wise gradient descent) and plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Plotting function\n",
    "# ==========================================================\n",
    "def plot_and_save_losses(train_losses, val_losses, filename=\"loss_curve.png\"):\n",
    "    \"\"\"Plots training and validation loss and saves the figure.\"\"\"\n",
    "    epochs = range(len(train_losses))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss', color='red')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss (Normalized)')\n",
    "    plt.yscale('log') # Use log scale for clearer visualization of small losses\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(os.path.join(save_dir, filename))\n",
    "        print(f\"Loss plot saved to {filename}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving plot: {e}\", flush=True)\n",
    "    plt.close()\n",
    "\n",
    "# ==========================================================\n",
    "# Set training paramters\n",
    "# ==========================================================\n",
    "model = PhysicsEmulator().to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=1000, eta_min=1e-7\n",
    "            )\n",
    "criterion = nn.HuberLoss()\n",
    "epochs = 500 \n",
    "patience = 100  # Number of epochs to wait for improvement before stopping\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# ==========================================================\n",
    "# Load training and validation tensors\n",
    "# ==========================================================\n",
    "# Ensure Mass and Radius are Torch Tensors\n",
    "if isinstance(y_train_norm, np.ndarray):\n",
    "    y_train_norm = torch.from_numpy(y_train_norm).to(torch.float32)\n",
    "if isinstance(y_val_norm, np.ndarray):\n",
    "    y_val_norm = torch.from_numpy(y_val_norm).to(torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(X_train_norm, y_train_norm), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_norm, y_val_norm), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ==========================================================\n",
    "# Training\n",
    "# ==========================================================\n",
    "for epoch in range(epochs):\n",
    "    # 1. Set model in training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_b, y_b in train_loader:\n",
    "        X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)\n",
    "        optimizer.zero_grad()       # Clear previous gradient\n",
    "        pred = model(X_b)           # Make prediction\n",
    "        loss = criterion(pred, y_b) # Calculate training loss\n",
    "        loss.backward()             # Backpropagate loss\n",
    "        optimizer.step()            # Use optimizer\n",
    "        train_loss += loss.item()   # Update training loss\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # 2. Set model in evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader:\n",
    "            X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)\n",
    "            pred = model(X_b)           # Make prediction\n",
    "            loss = criterion(pred, y_b) # Calculate validation loss\n",
    "            val_loss += loss.item()     # Update validation loss\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step()                    # Update Scheduler\n",
    "    \n",
    "    # 3. Append Losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # 4. Early Stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model weights\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break    \n",
    "        \n",
    "\n",
    "    # 5. Calculate and print error in output parameters every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        # Calculate the Approximate Physical Error in km\n",
    "        phys_error_km = np.sqrt(2 * val_loss) * RADIUS_SCALE \n",
    "        mass_error = np.sqrt(2 * val_loss) * MASS_SCALE\n",
    "        td_error = np.sqrt(2 * val_loss)  # Since td is log-scaled, this is in log units\n",
    "\n",
    "        print(f\"Epoch {epoch} | Val Loss: {val_loss:.6e} | Approx Radius Error: {phys_error_km:.4f} km | Approx Mass Error: {mass_error:.4f} | Approx TD error: {td_error:.4f}\", flush=True)\n",
    "\n",
    "        # 6. Plot and save errors\n",
    "        # Plot every 100 epochs\n",
    "        if epoch % 100 == 0 and epoch > 0:\n",
    "            plot_and_save_losses(train_losses, val_losses, filename=f\"loss_curve_epoch{epoch}.png\")\n",
    "\n",
    "# 7. Final plot after training finishes\n",
    "plot_and_save_losses(train_losses, val_losses, filename=\"loss_curve_final.png\")\n",
    "\n",
    "# 8. Saving the best model to be loaded later\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"Best_EOS_Model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a45c3",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Testing the model on Unseen files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da01a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 1. Configuration and constants\n",
    "# ==========================================================\n",
    "DATA_DIR_PATH = \"/home/kay/ML-for-Neutron-Stars/All_MR_Relations/All_MR_Relations/\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RADIUS_SCALE = 25.0\n",
    "MASS_SCALE = 3.5\n",
    "# How many unseen EOS to plot\n",
    "N_MR_FILES = 10         # for Mass–Radius plot <--------------------------------\n",
    "N_TD_FILES = 1          # for Compactness–Tidal plot <--------------------------\n",
    "\n",
    "MODEL_NO = 4            # For saving plots <------------------------------------\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Define function to extract EOS paramters from a file\n",
    "# ==========================================================\n",
    "def extract_eos_params(filename):\n",
    "    match = pattern.match(filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    # Extract raw values exactly like training\n",
    "    m = float(match.group(\"m\")) / 100.0\n",
    "    L = float(match.group(\"L\"))\n",
    "    J = float(match.group(\"J\"))\n",
    "    n_v = float(match.group(\"n_v\"))\n",
    "    d = float(match.group(\"d\"))\n",
    "    B = float(match.group(\"B\")) / 1000.0\n",
    "    n = float(match.group('n')) / 1000.0  # e.g., 0.160\n",
    "    \n",
    "    # Return as a numpy array for easy stacking [m, L, J, n_v, d, B, n]\n",
    "    return np.array([m, L, J, n_v, d, B, n], dtype=np.float32)\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Filter out the unseen files\n",
    "# ==========================================================\n",
    "# Regex must match the Training Pipeline exactly\n",
    "pattern = re.compile(\n",
    "    r\"MREoSm(?P<m>\\d+)\"\n",
    "    r\"L(?P<L>\\d+)\"\n",
    "    r\"J(?P<J>\\d+)\"\n",
    "    r\"T(?P<T>\\d+)_\"\n",
    "    r\"n(?P<n>\\d+)_\"\n",
    "    r\"Yp?\\+.*?RGgen_\"\n",
    "    r\"v(?P<n_v>[-\\d\\.]+)\"\n",
    "    r\"d(?P<d>[-\\d\\.]+)\"\n",
    "    r\"B(?P<Btype>[np])(?P<B>\\d+)\\.dat\"\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Select Unseen files and extract the input at output paramters\n",
    "# ==============================================================================\n",
    "# Path to data\n",
    "all_files = glob.glob(os.path.join(DATA_DIR_PATH, \"MR*.dat\"))\n",
    "print(\"Total files available:\", len(all_files))\n",
    "\n",
    "# 4.1. Load the list of files used for training\n",
    "# Ensure this file exists from the training run!\n",
    "try:\n",
    "    training_files_used = np.load(os.path.join(save_dir, \"files_used_for_training.npy\"), allow_pickle=True)\n",
    "    # Convert to set for fast lookup\n",
    "    training_set = {\n",
    "    os.path.basename(f) for f in training_files_used        \n",
    "    }\n",
    "    print(\"Length of training set:\", len(training_set))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: List of training files not found. Using random files (risk of data leakage).\")\n",
    "    training_set = {\n",
    "    os.path.basename(f) for f in training_files_used        \n",
    "    }\n",
    "    print(\"Length of random training set:\", len(training_set))\n",
    "\n",
    "# 4.2. Filter: Keep only files NOT in the training set\n",
    "test_pool = [\n",
    "    f for f in all_files\n",
    "    if os.path.basename(f) not in training_set\n",
    "]\n",
    "print(\"Total unseen files available for testing:\", len(test_pool))\n",
    "\n",
    "# 4.3. Ensure we have enough unseen files\n",
    "if len(test_pool) < max(N_MR_FILES, N_TD_FILES):\n",
    "    raise RuntimeError(\"Not enough unseen files available.\")\n",
    "\n",
    "# 4.4. Select the required number of files for the M-R plot randomly\n",
    "selected_files_MR = np.random.choice(\n",
    "    test_pool, size=N_MR_FILES, replace=False\n",
    ")\n",
    "\n",
    "# 4.4. Select the required number of files for the TD-C plot randomly\n",
    "selected_files_TD = np.random.choice(\n",
    "    test_pool, size=N_TD_FILES, replace=False\n",
    ")\n",
    "\n",
    "print(f\"MR plot   : {len(selected_files_MR)} unseen files\")\n",
    "print(f\"TD plot   : {len(selected_files_TD)} unseen files\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Prepare model and scalars\n",
    "# ==============================================================================\n",
    "# Load Model and call to device\n",
    "model = PhysicsEmulator(input_dim=8).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"Best_EOS_Model.pth\"), map_location=DEVICE))\n",
    "model.eval()    # Set model in evaluation mode\n",
    "\n",
    "# Load the means and stds for normalization from training\n",
    "if 'X_eos_mean' not in locals():\n",
    "    # Loading the SAME mean/std from TRAINING.\n",
    "    X_eos_mean = torch.load(os.path.join(save_dir, \"X_eos_mean.pt\"), map_location=DEVICE)\n",
    "    X_eos_std = torch.load(os.path.join(save_dir, \"X_eos_std.pt\"), map_location=DEVICE)\n",
    "    print(\"Using saved normalization statistics.\")\n",
    "else:\n",
    "    print(\"Using existing normalization statistics in memory.\")\n",
    "    X_eos_mean = X_eos_mean.to(DEVICE)\n",
    "    X_eos_std = X_eos_std.to(DEVICE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Evaluation and plot loop\n",
    "# ==============================================================================\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# M-R plot loop\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "colors_MR = plt.cm.jet(np.linspace(0, 1, len(selected_files_MR)))\n",
    "\n",
    "for color, file_path in zip(colors_MR, selected_files_MR):\n",
    "    # 1. Extract all EOS parameters from the chosen files\n",
    "    filename = os.path.basename(file_path)\n",
    "    eos_params = extract_eos_params(filename)\n",
    "    if eos_params is None:\n",
    "        continue\n",
    "\n",
    "    # load data\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # 2. Get cp, mass and radius values from within the file\n",
    "    cp_vals     = data[:, 0]    \n",
    "    mass_vals   = data[:, 1]\n",
    "    radius_vals = data[:, 2]\n",
    "\n",
    "    # 3. Fix to only use stable branch up to M_max - 2 points\n",
    "    max_m_idx = np.argmax(mass_vals)\n",
    "    cut_idx = max(1, max_m_idx - 2)\n",
    "    cp_vals     = cp_vals[:cut_idx]\n",
    "    mass_vals   = mass_vals[:cut_idx]\n",
    "    radius_vals = radius_vals[:cut_idx]\n",
    "\n",
    "    # 4. No Minimmum mass cutoff\n",
    "\n",
    "    # 5. Basic filtering to ensure positive values\n",
    "    valid_mask = (radius_vals > 0) & (mass_vals > 0)\n",
    "    cp_vals     = cp_vals[valid_mask]\n",
    "    mass_vals   = mass_vals[valid_mask]\n",
    "    radius_vals = radius_vals[valid_mask]\n",
    "\n",
    "    # 6. Prepare input tensors\n",
    "    # 6.1. Tile the EOS params for every mass point\n",
    "    num_points = len(mass_vals)\n",
    "    eos_repeated = np.tile(eos_params, (num_points, 1))\n",
    "\n",
    "    # 6.2. Convert to Tensor\n",
    "    X_eos_input  = torch.tensor(eos_repeated, dtype=torch.float32, device=DEVICE)\n",
    "    X_cp_input = torch.tensor(cp_vals[:, None], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # 6.3. Normalize using training statistics\n",
    "    # EOS: Z-score\n",
    "    X_eos_norm  = (X_eos_input - X_eos_mean) / X_eos_std\n",
    "    # CP: Log Scaling\n",
    "    X_cp_norm = torch.log10(X_cp_input)\n",
    "\n",
    "    # 6.4. Concatenate inputs\n",
    "    model_input = torch.cat((X_eos_norm, X_cp_norm), dim=1)\n",
    "\n",
    "    # 7. Predict and denormalize\n",
    "    with torch.no_grad():\n",
    "        pred = model(model_input)\n",
    "        M_pred = (pred[:, 0] * MASS_SCALE).cpu().numpy()\n",
    "        R_pred_km = (pred[:, 1] * RADIUS_SCALE).cpu().numpy()\n",
    "\n",
    "    # Label for plotting\n",
    "    label = (\n",
    "        f\"m={eos_params[0]:.2f}, L={eos_params[1]:.0f}, \"\n",
    "        f\"J={eos_params[2]:.0f}, n_v={eos_params[3]:.2f},\"\n",
    "        f\"d={eos_params[4]:.2f}, B={eos_params[5]:.3f}, \"\n",
    "        f\"n={eos_params[6]:.3f}, cp = {cp_vals[0]:.2f}\"\n",
    "    )\n",
    "\n",
    "    # 8. M-R Plot\n",
    "    ax1.plot(radius_vals, mass_vals, \"-\", color=color, alpha=0.4)\n",
    "    ax1.plot(R_pred_km, M_pred, \"--\", color=color, label=label)\n",
    "\n",
    "    # Formatting Plot1\n",
    "    ax1.set_title(\"Mass–Radius Curves: (TOV + Tabular EOS) vs (Model Predictions)\")\n",
    "    ax1.set_xlabel(\"Radius (km)\")\n",
    "    ax1.set_ylabel(r\"Mass ($M_\\odot$)\")\n",
    "    ax1.set_xlim(9, 20)\n",
    "    ax1.set_ylim(0, 3.5)\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TD-C plot loop\n",
    "# ------------------------------------------------------------------------------\n",
    "colors_TD = plt.cm.viridis(np.linspace(0, 1, len(selected_files_TD)))\n",
    "\n",
    "for color, file_path in zip(colors_TD, selected_files_TD):\n",
    "    # 1. Extract all EOS parameters from the chosen files\n",
    "    filename = os.path.basename(file_path)\n",
    "    eos_params = extract_eos_params(filename)\n",
    "    if eos_params is None:\n",
    "        continue\n",
    "\n",
    "    # load data\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # 2. Get cp, mass, radius and tidal deformability values from within the file\n",
    "    cp_vals     = data[:, 0]\n",
    "    mass_vals   = data[:, 1]\n",
    "    radius_vals = data[:, 2]\n",
    "    td_vals     = data[:, 3]\n",
    "\n",
    "    # 3. Fix to only use stable branch up to M_max - 2 points\n",
    "    max_m_idx = np.argmax(mass_vals)\n",
    "    cut_idx = max(1, max_m_idx - 2)\n",
    "    cp_vals     = cp_vals[:cut_idx]\n",
    "    mass_vals   = mass_vals[:cut_idx]\n",
    "    radius_vals = radius_vals[:cut_idx]\n",
    "    td_vals     = td_vals[:cut_idx]\n",
    "\n",
    "    # 4. No Minimmum mass cutoff\n",
    "\n",
    "    # 5. Basic filtering to ensure positive values\n",
    "    valid_mask = (radius_vals > 0) & (mass_vals > 0)\n",
    "    cp_vals     = cp_vals[valid_mask]\n",
    "    mass_vals   = mass_vals[valid_mask]\n",
    "    radius_vals = radius_vals[valid_mask]\n",
    "    td_vals     = td_vals[valid_mask]\n",
    "\n",
    "    # 6. Prepare input tensors\n",
    "    # 6.1. Tile the EOS params for every mass point\n",
    "    num_points = len(mass_vals)\n",
    "    eos_repeated = np.tile(eos_params, (num_points, 1))\n",
    "\n",
    "    # 6.2. Convert to Tensor\n",
    "    X_eos_input  = torch.tensor(eos_repeated, dtype=torch.float32, device=DEVICE)\n",
    "    X_cp_input   = torch.tensor(cp_vals[:, None], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # 6.3. Normalize using training statistics\n",
    "    # EOS: Z-score\n",
    "    X_eos_norm  = (X_eos_input - X_eos_mean) / X_eos_std\n",
    "    # CP: Log Scaling\n",
    "    X_cp_norm   = torch.log10(X_cp_input)\n",
    "\n",
    "    # 6.4. Concatenate inputs\n",
    "    model_input = torch.cat((X_eos_norm, X_cp_norm), dim=1)\n",
    "\n",
    "    # 7. Predict and denormalize\n",
    "    with torch.no_grad():\n",
    "        pred = model(model_input)\n",
    "        M_pred = (pred[:, 0] * MASS_SCALE).cpu().numpy()\n",
    "        R_pred_km = (pred[:, 1] * RADIUS_SCALE).cpu().numpy()\n",
    "        td_pred   = (10 ** pred[:, 2]).cpu().numpy()\n",
    "\n",
    "    # 8. Calculate compactness\n",
    "    compact_truth = mass_vals / radius_vals\n",
    "    compact_pred  = M_pred / R_pred_km\n",
    "\n",
    "    # 9. TD-C Plot\n",
    "    ax2.plot(compact_truth, np.log10(td_vals), \"-\", color=color, alpha=0.4,\n",
    "             label = \"TOV + Tabular EOS\")\n",
    "    ax2.plot(compact_pred,  np.log10(td_pred), \"--\", color=color,\n",
    "             label = f\"Model Prediction: {label}\")\n",
    "\n",
    "\n",
    "    # Formatting Plot2\n",
    "    ax2.set_title(\"Tidal Deformability-Compactness curve (TOV + Tabular EOS) vs (Model Predictions)\")\n",
    "    ax2.set_xlabel(r\"Compactness ($M/R$)\")\n",
    "    ax2.set_ylabel(\"Tidal Deformability (log$_{10}$)\")\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "# Legend Plot1\n",
    "fig1.legend(\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.18),\n",
    "    fontsize=\"small\",\n",
    "    ncol=2\n",
    ")\n",
    "fig1.subplots_adjust(bottom=0.28)\n",
    "\n",
    "# Legend Plot2\n",
    "fig2.legend(\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.18),\n",
    "    fontsize=\"small\",\n",
    "    ncol=2\n",
    ")\n",
    "fig2.subplots_adjust(bottom=0.05)\n",
    "\n",
    "# Save both the plots\n",
    "fig1.savefig(os.path.join(save_dir, f\"MR_unseen_{MODEL_NO}.svg\"), bbox_inches=\"tight\")\n",
    "fig2.savefig(os.path.join(save_dir, f\"TD_compactness_unseen_{MODEL_NO}.svg\"), bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd7d50",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be32bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# All original values\n",
    "orig_mass_all = mass_vals\n",
    "orig_radius_all = radius_vals\n",
    "orig_td_all = td_vals\n",
    "\n",
    "# All predicted values\n",
    "pred_mass_all = M_pred\n",
    "pred_radius_all = R_pred_km\n",
    "pred_td_all = td_pred\n",
    "\n",
    "# Mass Metrics\n",
    "mae_mass = mean_absolute_error(orig_mass_all, pred_mass_all)\n",
    "rmse_mass = np.sqrt(mean_squared_error(orig_mass_all, pred_mass_all))\n",
    "r2_mass = r2_score(orig_mass_all, pred_mass_all)\n",
    "\n",
    "# Radius Metrics\n",
    "mae_radius = mean_absolute_error(orig_radius_all, pred_radius_all)\n",
    "rmse_radius = np.sqrt(mean_squared_error(orig_radius_all, pred_radius_all))\n",
    "r2_radius = r2_score(orig_radius_all, pred_radius_all)\n",
    "\n",
    "# TD Metrics\n",
    "mae_td = mean_absolute_error(np.log10(orig_td_all), np.log10(pred_td_all))\n",
    "rmse_td = np.sqrt(mean_squared_error(np.log10(orig_td_all), np.log10(pred_td_all)))\n",
    "r2_td = r2_score(orig_td_all, pred_td_all)\n",
    "\n",
    "# Normalized MSE of the model for all three parameters combined\n",
    "norm_rmse = np.mean([\n",
    "    rmse_radius / np.mean(orig_radius_all),\n",
    "    rmse_mass / np.mean(orig_mass_all),\n",
    "    rmse_td / np.mean(orig_td_all)\n",
    "])\n",
    "\n",
    "print(f\"Radius:  MAE={mae_radius:.4f}, RMSE={rmse_radius:.4f}, R²={r2_radius:.4f}\")\n",
    "print(f\"Mass: MAE={mae_mass:.4f}, RMSE={rmse_mass:.4f}, R²={r2_mass:.4f}\")\n",
    "print(f\"Tidal Deformability: MAE={mae_td:.4f}, RMSE={rmse_td:.4f}, R²={r2_td:.4f}\")\n",
    "print(f\"Combined normalized RMSE = {norm_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Save Metrics to a Text file\n",
    "# ==========================================================\n",
    "output_file = os.path.join(save_dir, f\"Model_metrics_delete_{MODEL_NO}.txt\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Mass, Radius and TD prediction metrics\\n\")\n",
    "    f.write(\"======================================\\n\")\n",
    "\n",
    "    f.write(\"Mass Prediction Metrics:\\n\")\n",
    "    f.write(f\"MAE   : {mae_mass:.6f}\\n\")\n",
    "    f.write(f\"RMSE  : {rmse_mass:.6f}\\n\")\n",
    "    f.write(f\"R^2   : {r2_mass:.6f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Radius Prediction Metrics:\\n\")\n",
    "    f.write(f\"MAE   : {mae_radius:.6f}\\ n\")\n",
    "    f.write(f\"RMSE  : {rmse_radius:.6f}\\n\")\n",
    "    f.write(f\"R^2   : {r2_radius:.6f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Tidal Deformability Prediction Metrics:\\n\")\n",
    "    f.write(f\"MAE   : {mae_td:.6f}\\n\")\n",
    "    f.write(f\"RMSE  : {rmse_td:.6f}\\n\")\n",
    "    f.write(f\"R^2   : {r2_td:.6f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(f\"Norm RMSE : {norm_rmse:.6f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01a442",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Testing model on Jan-Erik's new file with different step size for input paramter B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fedc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & HELPERS\n",
    "# ==============================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RADIUS_SCALE = 25.0\n",
    "MASS_SCALE = 3.5\n",
    "\n",
    "save_dir = \"399700files_MRTD/\"\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Define function to extract EOS paramters from a file\n",
    "# ==========================================================\n",
    "def extract_eos_params(filename):\n",
    "    match = pattern.match(filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    # Extract raw values exactly like training\n",
    "    m = float(match.group(\"m\")) / 100.0\n",
    "    L = float(match.group(\"L\"))\n",
    "    J = float(match.group(\"J\"))\n",
    "    n_v = float(match.group(\"n_v\"))\n",
    "    d = float(match.group(\"d\"))\n",
    "    B = float(match.group(\"B\")) / 1000.0\n",
    "    n = float(match.group('n')) / 1000.0  # e.g., 0.160\n",
    "    \n",
    "    # Return as a numpy array for easy stacking [m, L, J, n_v, d, B, n]\n",
    "    return np.array([m, L, J, n_v, d, B, n], dtype=np.float32)\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Filter out the unseen files\n",
    "# ==========================================================\n",
    "# Regex must match the Training Pipeline exactly\n",
    "pattern = re.compile(\n",
    "    r\"MREoSm(?P<m>\\d+)\"\n",
    "    r\"L(?P<L>\\d+)\"\n",
    "    r\"J(?P<J>\\d+)\"\n",
    "    r\"T(?P<T>\\d+)_\"\n",
    "    r\"n(?P<n>\\d+)_\"\n",
    "    r\"Yp0f?\\+.*?RGgen_\"\n",
    "    r\"v(?P<n_v>[-\\d\\.]+)\"\n",
    "    r\"d(?P<d>[-\\d\\.]+)\"\n",
    "    r\"B(?:(?P<Btype>[np]))?(?P<B>\\d+)\\.dat\"\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Select Unseen files and extract the input at output paramters\n",
    "# ==============================================================================\n",
    "# Path to data\n",
    "DATA_DIR_PATH = \"/home/kay/ML-for-Neutron-Stars/New files\"\n",
    "\n",
    "selected_files = glob.glob(os.path.join(DATA_DIR_PATH, \"MR*.dat\"))\n",
    "print(f\"Testing on {len(selected_files)} unseen files.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Prepare model and scalars\n",
    "# ==============================================================================\n",
    "# Load Model and call to device\n",
    "model = PhysicsEmulator(input_dim=8).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"Best_EOS_Model.pth\"), map_location=DEVICE))\n",
    "model.eval() # Put model in evaluation mode\n",
    "\n",
    "# Load the means and stds for normalization from training\n",
    "if 'X_eos_mean' not in locals():\n",
    "        # Loading the SAME mean/std from TRAINING.\n",
    "    X_eos_mean = torch.load(os.path.join(save_dir, \"X_eos_mean.pt\"), map_location=DEVICE)\n",
    "    X_eos_std = torch.load(os.path.join(save_dir, \"X_eos_std.pt\"), map_location=DEVICE)\n",
    "    print(\"ERROR: X_eos_mean and X_eos_std are missing!\")\n",
    "    print(\"You must save these during training or run this cell in the same notebook.\")\n",
    "else:\n",
    "    X_eos_mean = X_eos_mean.to(DEVICE)\n",
    "    X_eos_std = X_eos_std.to(DEVICE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Evaluation and plot loop\n",
    "# ==============================================================================\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(selected_files)))\n",
    "\n",
    "for color, file_path in zip(colors, selected_files):\n",
    "    # 1. Extract all EOS parameters from the chosen files\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(\"Processing file:\", filename)\n",
    "    eos_params = extract_eos_params(filename)\n",
    "    print(\"EOS params:\", len(eos_params) if eos_params is not None else \"None\")\n",
    "    \n",
    "    if eos_params is None: continue\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "        print(\"Data loaded!\")\n",
    "    except: continue\n",
    "        \n",
    "    # 2. Get cp, mass and radius values from within the file\n",
    "    mass_vals = data[:, 1]\n",
    "    radius_vals = data[:, 2]\n",
    "    cp_vals = data[:, 0]\n",
    "\n",
    "    # 3. Filter stable branch (Up to Max Mass) same as training\n",
    "    max_m_idx = np.argmax(mass_vals)\n",
    "    # Apply the same safety margin cut as training\n",
    "    SAFETY_MARGIN = 2\n",
    "    cut_idx = max(1, max_m_idx - SAFETY_MARGIN) \n",
    "    mass_vals = mass_vals[:cut_idx]\n",
    "    radius_vals = radius_vals[:cut_idx]\n",
    "    cp_vals = cp_vals[:cut_idx]\n",
    "\n",
    "    # 4. No Minimmum mass cutoff\n",
    "\n",
    "    # 5. Basic filtering to ensure positive values\n",
    "    positive_mask = (mass_vals > 0) & (radius_vals > 0)\n",
    "    mass_vals = mass_vals[positive_mask]\n",
    "    radius_vals = radius_vals[positive_mask]\n",
    "    cp_vals = cp_vals[positive_mask]\n",
    "\n",
    "    # 6. Prepare input tensors\n",
    "    # 6.1. Tile the EOS params for every mass point\n",
    "    num_points = len(mass_vals)\n",
    "    eos_repeated = np.tile(eos_params, (num_points, 1)) # Shape (N, 7)\n",
    "    \n",
    "    # 6.2. Convert to Tensor\n",
    "    X_eos_input = torch.tensor(eos_repeated, dtype=torch.float32).to(DEVICE)\n",
    "    X_cp_input = torch.tensor(cp_vals.reshape(-1, 1), dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    # 6.3. Normalize using training statistics\n",
    "    # EOS: Z-score\n",
    "    X_eos_norm = (X_eos_input - X_eos_mean) / X_eos_std\n",
    "    # CP: Log10 Scaling\n",
    "    X_cp_norm   = torch.log10(X_cp_input)\n",
    "    \n",
    "    # 6.4. Concatenate Inputs\n",
    "    model_input = torch.cat((X_eos_norm, X_cp_norm), dim=1)\n",
    "    \n",
    "    # 7. Predict and denormalize\n",
    "    with torch.no_grad():\n",
    "        # Predict normalized Mass and Radius (0.0 to ~1.0)\n",
    "        pred = model(model_input)\n",
    "        Mass_pred_norm = pred[:, 0:1]\n",
    "        R_pred_norm = pred[:, 1:2]\n",
    "\n",
    "        # De-normalize\n",
    "        R_pred = R_pred_norm * RADIUS_SCALE\n",
    "        Mass_pred = Mass_pred_norm * MASS_SCALE\n",
    "        \n",
    "        # Move to CPU for plotting\n",
    "        Mass_pred = Mass_pred.cpu().numpy().flatten()\n",
    "        R_pred= R_pred.cpu().numpy().flatten()\n",
    "\n",
    "    # 9. Plotting\n",
    "    label_txt = f\"\"\"m={eos_params[0]:.2f}, L={eos_params[1]:.0f}, \n",
    "                J={eos_params[2]:.0f}, n_v={eos_params[3]:.2f},\n",
    "                d={eos_params[4]:.2f}, B={eos_params[5]:.2f},\n",
    "                n={eos_params[6]:.3f}\"\"\" \n",
    "    \n",
    "    # Plot Original values\n",
    "    plt.plot(radius_vals, mass_vals, \"-\", color=color, alpha=0.5, linewidth=2)\n",
    "    \n",
    "    # Plot Prediction \n",
    "    plt.plot(R_pred, Mass_pred, \"--\", color=color, linewidth=2, label=label_txt)\n",
    " \n",
    "plt.title(\"Neural Network vs. TOV Solver (Unseen Files)\")\n",
    "plt.xlabel(\"Radius (km)\")\n",
    "plt.ylabel(r\"Mass ($M_{\\odot}$)\")\n",
    "plt.legend(title=\"Predictions\", fontsize='small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(9, 20)\n",
    "plt.ylim(0, 3.5)\n",
    "# plt.savefig(os.path.join(save_dir, \"Testing plot B.png\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f634d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All original values\n",
    "orig_mass_all = mass_vals\n",
    "orig_radius_all = radius_vals\n",
    "\n",
    "# All predicted values\n",
    "pred_mass_all = Mass_pred\n",
    "pred_radius_all = R_pred\n",
    "\n",
    "# Mass metrics\n",
    "mae_mass = mean_absolute_error(orig_mass_all, pred_mass_all)\n",
    "rmse_mass = np.sqrt(mean_squared_error(orig_mass_all, pred_mass_all))\n",
    "r2_mass = r2_score(orig_mass_all, pred_mass_all)\n",
    "\n",
    "# radius Metrics\n",
    "mae_radius = mean_absolute_error(orig_radius_all, pred_radius_all)\n",
    "rmse_radius = np.sqrt(mean_squared_error(orig_radius_all, pred_radius_all))\n",
    "r2_radius = r2_score(orig_radius_all, pred_radius_all)\n",
    "\n",
    "# Normalized MSE of the model for all three parameters combined\n",
    "norm_rmse = np.mean([\n",
    "    rmse_radius / np.mean(orig_radius_all),\n",
    "    rmse_mass / np.mean(orig_mass_all),\n",
    "])\n",
    "\n",
    "print(f\"Radius:  MAE={mae_radius:.4f}, RMSE={rmse_radius:.4f}, R²={r2_radius:.4f}\")\n",
    "print(f\"Mass:  RMSE={rmse_mass:.4f}, R²={r2_mass:.4f}\")\n",
    "print(f\"Combined normalized RMSE = {norm_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfcd94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
