{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8baa8cf",
   "metadata": {},
   "source": [
    "# TOV Emulator to train a NN to predict mass and radius of a Neutron Strar from a PP EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d294ab0",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Import all the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e019f",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Global constants and unit conversion factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ef6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "msun=147660                 # Solar mass in cm given by the formula G*M_sun/c^2\n",
    "\n",
    "c=2.9979e10                 # speed of light in cm/s (CGS)\n",
    "G=6.67408e-8                # gravitational constant in cm^3/gm/s^2 (CGS)\n",
    "\n",
    "dkm = 1.3234e-06            # conversion of MeV/fm^3 to km^-2\n",
    "dcgs = 1.78e12              # factor to convert from MeV/fm^3 to gm/cm^3\n",
    "conv = 8.2601e-40           # dyn/cm^2 to km^-2\n",
    "cgs1=1.7827e+12             # MeV/fm3 to gms/cm3\n",
    "cgs2=1.6022e+33             # MeV/fm3 to dyne/cm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d06afa",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Piecewise-polytrope low-density (crust) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b87cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polytropic exponents\n",
    "GammaL_1 = 1.35692\n",
    "GammaL_2 = 0.62223\n",
    "GammaL_3 = 1.28733\n",
    "GammaL_4 = 1.58425\n",
    "\n",
    "# Polytropic constants\n",
    "KL_1 = 3.99874e-8 # * pow(Msun/Length**3, GammaL_1-1)\n",
    "KL_2 = 5.32697e+1 # * pow(Msun/Length**3, GammaL_2-1)\n",
    "KL_3 = 1.06186e-6 # * pow(Msun/Length**3, GammaL_3-1)\n",
    "KL_4 = 6.80110e-9 # * pow(Msun/Length**3, GammaL_4-1)\n",
    "# notice a missing c^2 in Ki values in Table II of Read et al. 2009\n",
    "\n",
    "# Densities at the boundaries of the piecewise polytropes\n",
    "rhoL_1 = 2.62789e12\n",
    "rhoL_2 = 3.78358e11\n",
    "rhoL_3 = 2.44034e7\n",
    "rhoL_4 = 0.0\n",
    "\n",
    "# Pressures at the boundaries of the piecewise polytropes\n",
    "pL_1 = KL_1*rhoL_1**GammaL_1\n",
    "pL_2 = KL_2*rhoL_2**GammaL_2\n",
    "pL_3 = KL_3*rhoL_3**GammaL_3\n",
    "pL_4 = 0.0\n",
    "\n",
    "# The exact numbers are taken from a particular crust model/table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db785618",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Few more calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b0b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the offsets specific internal energy (epsL_i) and alphaL_i at the boundaries\n",
    "# The general form used: ε(ρ)=(1+α)ρ+K/(Γ−1)ρ^Γ. Solving for alpha ensures matching across boundaries.\n",
    "# Energy density needs an additive offset to enforce continuity.\n",
    "\n",
    "epsL_4 = 0.0\n",
    "alphaL_4 = 0.0\n",
    "epsL_3 = (1+alphaL_4)*rhoL_3 + KL_4/(GammaL_4 - 1.)*pow(rhoL_3, GammaL_4)\n",
    "alphaL_3 = epsL_3/rhoL_3 - 1.0 - KL_3/(GammaL_3 - 1.)*pow(rhoL_3, GammaL_3 -1.0)\n",
    "epsL_2 = (1+alphaL_3)*rhoL_2 + KL_3/(GammaL_3 - 1.)*pow(rhoL_2, GammaL_3)\n",
    "alphaL_2 = epsL_2/rhoL_2 - 1.0 - KL_2/(GammaL_2 - 1.)*pow(rhoL_2, GammaL_2 -1.0)\n",
    "epsL_1 = (1+alphaL_2)*rhoL_1 + KL_2/(GammaL_2 - 1.)*pow(rhoL_1, GammaL_2)\n",
    "alphaL_1 = epsL_1/rhoL_1 - 1.0 - KL_1/(GammaL_1 - 1.)*pow(rhoL_1, GammaL_1 -1.0)\n",
    "\n",
    "# Density thresholds for high-density polytropes\n",
    "rho1 = pow(10,14.7) # Break Density\n",
    "rho2 = pow(10,15.0) # Break Density\n",
    "\n",
    "# GR conversion prefactors to go from cgs pressure/energy-density units into geometric units (where G=c=1)\n",
    "t_p=G/c**4\n",
    "t_rho=G/c**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2952fe",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Forward EOS: ρ↦(p,ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c249b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_eps_of_rho(rho,logp,Gamma1,Gamma2,Gamma3):\n",
    "    p1 = pow(10.0,logp)/c**2\n",
    "    K1 = p1/pow(rho1,Gamma1)\n",
    "    K2 = K1 * pow( rho1, Gamma1-Gamma2)\n",
    "    K3 = K2 * pow( rho2, Gamma2-Gamma3)\n",
    "    rho0 = pow(KL_1/K1,1.0/(Gamma1-GammaL_1)) \n",
    "    eps0 = (1.0+alphaL_1)*rho0 + KL_1/(GammaL_1-1.0)*pow(rho0,GammaL_1)\n",
    "    alpha1 = eps0/rho0 - 1.0 - K1/(Gamma1 - 1.0)*pow(rho0, Gamma1 -1.0)\n",
    "    eps1 = (1.0+alpha1)*rho1 + K1/(Gamma1 - 1.0)*pow(rho1, Gamma1)\n",
    "    alpha2 = eps1/rho1 - 1.0 - K2/(Gamma2 - 1.0)*pow(rho1, Gamma2 -1.0)\n",
    "    eps2 = (1.0+alpha2)*rho2 + K2/(Gamma2 - 1.0)*pow(rho2, Gamma2)\n",
    "    alpha3 = eps2/rho2 - 1.0 - K3/(Gamma3 - 1.0)*pow(rho2, Gamma3 -1.0)\n",
    "    if rho<rhoL_3:\n",
    "        p = KL_4*pow(rho,GammaL_4)\n",
    "        eps = (1.0+alphaL_4)*rho + KL_4/(GammaL_4-1.0)*pow(rho,GammaL_4)\n",
    "    elif rhoL_3<= rho <rhoL_2:\n",
    "        p = KL_3*pow(rho,GammaL_3)\n",
    "        eps = (1.0+alphaL_3)*rho + KL_3/(GammaL_3-1.0)*pow(rho,GammaL_3)\n",
    "    elif rhoL_2<= rho <rhoL_1:\n",
    "        p = KL_2*pow(rho,GammaL_2)\n",
    "        eps = (1.0+alphaL_2)*rho + KL_2/(GammaL_2-1.0)*pow(rho,GammaL_2)\n",
    "    elif rhoL_1<= rho <rho0:\n",
    "        p = KL_1*pow(rho,GammaL_1)\n",
    "        eps = (1.0+alphaL_1)*rho + KL_1/(GammaL_1-1.0)*pow(rho,GammaL_1)\n",
    "    elif rho0<= rho <rho1:\n",
    "        p = K1*pow(rho,Gamma1)\n",
    "        eps = (1.0+alpha1)*rho + K1/(Gamma1-1.0)*pow(rho,Gamma1)\n",
    "    elif rho1<= rho <rho2:\n",
    "        p = K2*pow(rho,Gamma2)\n",
    "        eps = (1.0+alpha2)*rho + K2/(Gamma2-1.0)*pow(rho,Gamma2)\n",
    "    else:\n",
    "        p = K3*pow(rho,Gamma3)\n",
    "        eps = (1.0+alpha3)*rho + K3/(Gamma3-1.0)*pow(rho,Gamma3)\n",
    "    return p*c**2, eps*c**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55c8f7",
   "metadata": {},
   "source": [
    "### <div style= 'color: goldenrod'> Inverse EOS: p↦ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa859b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def eps(p,logp,Gamma1,Gamma2,Gamma3):\n",
    "    p1 = pow(10.0,logp)/c**2\n",
    "    p*=1/c**2\n",
    "    K1 = p1/pow(rho1,Gamma1)\n",
    "    K2 = K1 * pow( rho1, Gamma1-Gamma2)\n",
    "    K3 = K2 * pow( rho2, Gamma2-Gamma3)\n",
    "    rho0 = pow(KL_1/K1,1.0/(Gamma1-GammaL_1))\n",
    "    eps0 = (1.0+alphaL_1)*rho0 + KL_1/(GammaL_1-1.0)*pow(rho0,GammaL_1)\n",
    "    alpha1 = eps0/rho0 - 1.0 - K1/(Gamma1 - 1.0)*pow(rho0, Gamma1 -1.0)\n",
    "    eps1 = (1.0+alpha1)*rho1 + K1/(Gamma1 - 1.0)*pow(rho1, Gamma1)\n",
    "    alpha2 = eps1/rho1 - 1.0 - K2/(Gamma2 - 1.0)*pow(rho1, Gamma2 -1.0)\n",
    "    eps2 = (1.0+alpha2)*rho2 + K2/(Gamma2 - 1.0)*pow(rho2, Gamma2)\n",
    "    alpha3 = eps2/rho2 - 1.0 - K3/(Gamma3 - 1.0)*pow(rho2, Gamma3 -1.0)\n",
    "    p0 = K1*pow(rho0,Gamma1)\n",
    "    p2 = K3*pow(rho2,Gamma3)\n",
    "    if  p<pL_3:\n",
    "        rho = pow(p/KL_4,1/GammaL_4)\n",
    "        eps = (1.0+alphaL_4)*rho + KL_4/(GammaL_4-1.0)*pow(rho,GammaL_4)\n",
    "    elif pL_3<= p <pL_2:\n",
    "        rho = pow(p/KL_3,1/GammaL_3)\n",
    "        eps = (1.0+alphaL_3)*rho + KL_3/(GammaL_3-1.0)*pow(rho,GammaL_3)\n",
    "    elif pL_2<= p <pL_1:\n",
    "        rho = pow(p/KL_2,1/GammaL_2)\n",
    "        eps = (1.0+alphaL_2)*rho + KL_2/(GammaL_2-1.0)*pow(rho,GammaL_2)\n",
    "    elif  pL_1<p <p0:\n",
    "        rho = pow(p/KL_1,1/GammaL_1)\n",
    "        eps = (1.0+alphaL_1)*rho + KL_1/(GammaL_1-1.0)*pow(rho,GammaL_1)\n",
    "    elif p0<= p <p1:\n",
    "        rho = pow(p/K1,1/Gamma1)\n",
    "        eps = (1.0+alpha1)*rho + K1/(Gamma1-1.0)*pow(rho,Gamma1)\n",
    "    elif p1<= p <p2:\n",
    "        rho = pow(p/K2,1/Gamma2)\n",
    "        eps = (1.0+alpha2)*rho + K2/(Gamma2-1.0)*pow(rho,Gamma2)\n",
    "    else:\n",
    "        rho = pow(p/K3,1/Gamma3)\n",
    "        eps = (1.0+alpha3)*rho + K3/(Gamma3-1.0)*pow(rho,Gamma3)\n",
    "    return eps*c**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761b134",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> The TOV Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78fd8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOV(logrho_c,theta):\n",
    "    seq=[]\n",
    "    logp,Gamma1,Gamma2,Gamma3 = theta[0],theta[1],theta[2],theta[3]\n",
    "    dr=100\n",
    "    rho_c = pow(10,logrho_c)\n",
    "    m=0.0\n",
    "    r=0.1\n",
    "    p,e = p_eps_of_rho(rho_c,theta[0],theta[1],theta[2],theta[3])\n",
    "    p*=t_p\n",
    "    e*=t_p\n",
    "       \n",
    "    while p >= 0:\n",
    "        p += -(e+p)*(m+4.0*np.pi*(r**3.0)*p)*dr/(r*(r-2.0*m))\n",
    "        if p <= 0:\n",
    "            break               \n",
    "        m += 4.0*np.pi*(r**2.0)*e*dr\n",
    "        r += dr\n",
    "        e = eps(p/t_p,theta[0],theta[1],theta[2],theta[3])*t_p\n",
    "            \n",
    "    return  m/msun,r/1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d83c70e",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Produce a mass-radius curve for a single EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ca8ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "logrho_c_array=np.linspace(14.5,15.4,100)\n",
    "theta=[34.495, 3.446, 3.572, 2.887] #MPA1\n",
    "\n",
    "seq=[]\n",
    "for logrho_c in logrho_c_array:\n",
    "    m,r=TOV(logrho_c,theta)\n",
    "    seq.append((m,r))\n",
    "    \n",
    "seq=np.vstack(seq)\n",
    "\n",
    "plt.plot(seq[:,1],seq[:,0])\n",
    "plt.grid()\n",
    "plt.xlabel('Radius (km)')\n",
    "plt.ylabel(r'$M \\, (M_{\\odot})$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46e359",
   "metadata": {},
   "source": [
    "### <div style= 'color: slateblue'>Create dataset of EOS parameter samples & run TOV to create targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of EOS samples\n",
    "num_samples = 10000  \n",
    "\n",
    "EOS_params = np.random.uniform(low=[1.4, 1.4], \n",
    "                                   high=[5., 5.], \n",
    "                                   size=(num_samples, 2))\n",
    "\n",
    "# Sample central pressure values (in dyn/cm²)\n",
    "logrho_c_samples = np.random.uniform(low=14.5, high=15.4, size=(num_samples, 1))\n",
    "\n",
    "# Placeholder for Mass, Radius, Tidal Deformability\n",
    "MR_data = []\n",
    "\n",
    "# Generate dataset\n",
    "for i in range(num_samples):\n",
    "    logrho_c = logrho_c_samples[i, 0]\n",
    "    params = EOS_params[i]\n",
    "    M,R = TOV(logrho_c,[34.495, 3.446,params[0],params[1]])\n",
    "    MR_data.append([M,R])\n",
    "\n",
    "# Convert to NumPy array\n",
    "EOS_data = np.hstack([logrho_c_samples, EOS_params, np.array(MR_data)])\n",
    "\n",
    "# Save dataset\n",
    "np.save(\"EOS_dataset.npy\", EOS_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab549a9b",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> PyTorch ML pipeline — load & prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08828ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = np.load(\"EOS_dataset.npy\")\n",
    "print(data[1:10])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Convert to pytorch tensors\n",
    "X = torch.tensor(data[ : , :3], dtype=torch.float32)  # log_rho_c,  Gamma2, Gamma3\n",
    "y = torch.tensor(data[ : , 3:5], dtype=torch.float32)  # Mass, Radius\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Compute mean and std only on the training dataset\n",
    "X_mean, X_std = X_train.mean(dim=0), X_train.std(dim=0)\n",
    "y_mean, y_std = y_train.mean(dim=0), y_train.std(dim=0)\n",
    "\n",
    "# Normalize both the training and validation datasets\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_val_norm   = (X_val - X_mean) / X_std\n",
    "\n",
    "y_train_norm = (y_train - y_mean) / y_std\n",
    "y_val_norm   = (y_val - y_mean) / y_std\n",
    "\n",
    "# Reshape the tensors for dataloader\n",
    "X_train_norm, y_train_norm = X_train_norm.view(-1, 3), y_train_norm.view(-1, 2)\n",
    "X_val_norm, y_val_norm = X_val_norm.view(-1, 3), y_val_norm.view(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca4808",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> NN Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ed1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network\n",
    "class PP_NN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=50, output_dim=2):\n",
    "        super(PP_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d32c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_NN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd6518",
   "metadata": {},
   "source": [
    "### <div style= 'color: Slateblue'> Train the model (full-batch gradient descent) and plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d26496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = PP_NN()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "epochs = 5000\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_norm)\n",
    "    loss = criterion(outputs, y_train_norm)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    training_losses.append(loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_norm)\n",
    "        val_loss = criterion(val_outputs, y_val_norm)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"\"\"Epoch [{epoch + 1}/{epochs}],\n",
    "              Training Loss: {loss.item():.4f},\n",
    "              Validation Loss: {val_loss.item():.4f}\"\"\")\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"PP_nn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998527e",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Load the trained model for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f34c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model class\n",
    "model = PP_NN()\n",
    "# Load the saved state dict\n",
    "model.load_state_dict(torch.load(f\"/home/kay/ML-for-Neutron-Stars/PP_emulator/PP_TOV_Emulator_2_model2.pth\"))\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba153b0",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Evaluate / compare TOV vs NN predictions for grid of Gamma2/Gamma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5adfa942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare ranges for Gamma2 and Gamma3\n",
    "Gamma2_values = [2., 3., 4.]  \n",
    "Gamma3_values = [2., 3., 4.]  \n",
    "\n",
    "# Prepare the array for logrho_c values\n",
    "logrho_c_array = np.linspace(14.5, 15.4, 100)\n",
    "\n",
    "# Initialize lists to store results\n",
    "predicted_mass = []\n",
    "predicted_radius = []\n",
    "original_mass = []\n",
    "original_radius = []\n",
    "\n",
    "# Iterate over all logp and Gamma1 values\n",
    "for Gamma2 in Gamma2_values:\n",
    "    for Gamma3 in Gamma3_values:\n",
    "        # Store results for this logp and Gamma1 combination\n",
    "        predicted_mass_single = []\n",
    "        predicted_radius_single = []\n",
    "        original_mass_single = []\n",
    "        original_radius_single = []\n",
    "\n",
    "        for logrho_c in logrho_c_array:\n",
    "            # Prepare input tensor for prediction\n",
    "            input_tensor = torch.tensor([[logrho_c, Gamma2, Gamma3]], dtype=torch.float32)\n",
    "            \n",
    "            # Normalize the input data using the training mean and std\n",
    "            input_norm = (input_tensor - X_mean) / X_std\n",
    "            \n",
    "            # Make predictions with the trained model\n",
    "            with torch.no_grad():\n",
    "                prediction = model(input_norm)\n",
    "            \n",
    "            # Convert the prediction tensor to NumPy and denormalize the predicted mass and radius\n",
    "            prediction_np = prediction.detach().numpy()[0]  # Convert to NumPy and get the first element\n",
    "            \n",
    "            # Denormalize the predicted mass and radius\n",
    "            mass_pred, radius_pred = prediction_np * y_std.numpy() + y_mean.numpy()\n",
    "            \n",
    "            # Calculate original mass and radius using TOV\n",
    "            M_orig, R_orig = TOV(logrho_c, [34.495, 3.446, Gamma2, Gamma3])\n",
    "            \n",
    "            # Store the results\n",
    "            predicted_mass_single.append(mass_pred)\n",
    "            predicted_radius_single.append(radius_pred)\n",
    "            original_mass_single.append(M_orig)\n",
    "            original_radius_single.append(R_orig)\n",
    "\n",
    "        # After iterating over logrho_c, store the results for each kappa, gamma pair\n",
    "        predicted_mass.append(predicted_mass_single)\n",
    "        predicted_radius.append(predicted_radius_single)\n",
    "        original_mass.append(original_mass_single)\n",
    "        original_radius.append(original_radius_single)\n",
    "\n",
    "# Plotting Mass vs Radius for each kappa, gamma combination\n",
    "plt.figure(figsize=(11, 6))\n",
    "\n",
    "# Create a color map\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(Gamma2_values) * len(Gamma3_values)))\n",
    "\n",
    "# Iterate and assign the same color to TOV and NN prediction\n",
    "for i, (Gamma2, Gamma3) in enumerate([(k, g) for k in Gamma2_values for g in Gamma3_values]):\n",
    "    color = colors[i]\n",
    "    plt.plot(original_radius[i], original_mass[i], label=f\"TOV (Gamma2={Gamma2}, Gamma3={Gamma3})\", linestyle='-', color=color)\n",
    "    plt.plot(predicted_radius[i], predicted_mass[i], label=f\"NN (Gamma2={Gamma2}, Gamma3={Gamma3})\", linestyle='--', color=color)\n",
    "\n",
    "plt.xlabel(\"Radius (km)\")\n",
    "plt.ylabel(\"Mass (M_sun)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='best', borderaxespad=0)\n",
    "plt.title(\"Comparison of TOV and Neural Network Predictions for Various Gamma2 and Gamma3 Values\")\n",
    "plt.tight_layout(rect=[0, 0, 0.6, 1])\n",
    "plt.grid()\n",
    "# plt.savefig(\"PP_TOV_Emulator_2_plot(Gamma2_Gamma3).png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b334c",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1aaaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "orig_mass_all = np.concatenate(original_mass)\n",
    "pred_mass_all = np.concatenate(predicted_mass)\n",
    "orig_radius_all = np.concatenate(original_radius)\n",
    "pred_radius_all = np.concatenate(predicted_radius)\n",
    "\n",
    "mae_mass = mean_absolute_error(orig_mass_all, pred_mass_all)\n",
    "rmse_mass = np.sqrt(mean_squared_error(orig_mass_all, pred_mass_all))\n",
    "r2_mass = r2_score(orig_mass_all, pred_mass_all)\n",
    "\n",
    "mae_radius = mean_absolute_error(orig_radius_all, pred_radius_all)\n",
    "rmse_radius = np.sqrt(mean_squared_error(orig_radius_all, pred_radius_all))\n",
    "r2_radius = r2_score(orig_radius_all, pred_radius_all)\n",
    "\n",
    "norm_rmse = np.mean([\n",
    "    rmse_mass / np.mean(orig_mass_all),\n",
    "    rmse_radius / np.mean(orig_radius_all)\n",
    "])\n",
    "\n",
    "print(f\"Mass:    MAE={mae_mass:.4f}, RMSE={rmse_mass:.4f}, R²={r2_mass:.4f}\")\n",
    "print(f\"Radius:  MAE={mae_radius:.4f}, RMSE={rmse_radius:.4f}, R²={r2_radius:.4f}\")\n",
    "print(f\"Combined normalized RMSE = {norm_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038606f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
