{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50b410b",
   "metadata": {},
   "source": [
    "## <div style= 'color: White'> This is the newer, updated and more sophisticated version of the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08120107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418ae44",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Create dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6e349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Training Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train: 100%|██████████| 319760/319760 [18:16<00:00, 291.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Validation Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Val: 100%|██████████| 79940/79940 [06:02<00:00, 220.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!\n",
      "(319760,)\n",
      "(79940,)\n",
      "Datasets saved!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "NUM_FILES_TO_USE = 399700 # Increased for better generalization\n",
    "DATA_DIR_PATH = \"/home/kay/ML-for-Neutron-Stars/3_PP_EOS_emulator_real_data/All_MR_Relations/All_MR_Relations/\"\n",
    "BATCH_SIZE = 256 # Larger batch size for smoother gradients\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Regex to extract parameters\n",
    "pattern = re.compile(\n",
    "    r\"MREoSm(?P<m>\\d+)\"\n",
    "    r\"L(?P<L>\\d+)\"\n",
    "    r\"J(?P<J>\\d+)\"\n",
    "    r\"T(?P<T>\\d+)_\"\n",
    "    r\"n(?P<n>\\d+)_\"\n",
    "    r\"Yp?\\+.*?RGgen_\"\n",
    "    r\"v(?P<n_v>[-\\d\\.]+)\"\n",
    "    r\"d(?P<d>[-\\d\\.]+)\"\n",
    "    r\"B(?P<Btype>[np])(?P<B>\\d+)\\.dat\"\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA PROCESSING (SPLIT BY FILE)\n",
    "# ==============================================================================\n",
    "all_files = glob.glob(os.path.join(DATA_DIR_PATH, \"MR*.dat\"))\n",
    "if not all_files: raise RuntimeError(\"No files found.\")\n",
    "\n",
    "# Shuffle FILES first, then split\n",
    "np.random.shuffle(all_files)\n",
    "selected_files = all_files[:NUM_FILES_TO_USE]\n",
    "\n",
    "# Split files into Train and Validation lists (80/20 split)\n",
    "split_idx = int(0.8 * len(selected_files))\n",
    "train_files = selected_files[:split_idx]\n",
    "val_files = selected_files[split_idx:]\n",
    "\n",
    "def process_files(file_list, label_desc):\n",
    "    dataset_rows = []\n",
    "    files_used_for_training = []\n",
    "    \n",
    "    for file_path in tqdm(file_list, desc=f\"Processing {label_desc}\"):\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = pattern.match(filename)\n",
    "        if not match: continue\n",
    "\n",
    "        files_used_for_training.append(file_path)\n",
    "\n",
    "        # --- FIX 1: Extract ALL parameters ---\n",
    "        m = float(match.group(\"m\")) / 100.0 \n",
    "        L = float(match.group(\"L\"))\n",
    "        J = float(match.group(\"J\"))             \n",
    "        T = float(match.group(\"T\")) # Unused but extracted for completeness             \n",
    "        n = float(match.group('n')) / 1000.0\n",
    "        n_v = float(match.group(\"n_v\"))\n",
    "        d = float(match.group(\"d\"))\n",
    "        B = float(match.group(\"B\")) / 1000.0\n",
    "        \n",
    "        try:\n",
    "            data = np.loadtxt(file_path)\n",
    "        except: continue\n",
    "\n",
    "        if data.ndim == 1 or data.shape[1] < 3: continue\n",
    "\n",
    "        # Columns: [Central pressure (P_c), Mass, Radius, Tidal Deformability, R_quarkcore, Baryonic mass (rho)]\n",
    "        cp = data[:, 0]\n",
    "        mass = data[:, 1]\n",
    "        radius = data[:, 2]\n",
    "\n",
    "        # --- Filter Unstable Branch ---\n",
    "        # We only want data up to the Maximum Mass. \n",
    "        # Usually data is sorted by Pc. M increases then decreases.\n",
    "        max_m_index = np.argmax(mass)\n",
    "        \n",
    "        # --- Truncate BEFORE the noisy peak at M_max ---\n",
    "        # Drop the last 2 points from the calculated stable branch\n",
    "        SAFETY_MARGIN = 2 \n",
    "        \n",
    "        # Ensure we don't end up with negative index\n",
    "        end_index = max(1, max_m_index + 1 - SAFETY_MARGIN)\n",
    "        \n",
    "        # Slice arrays to only keep stable, safe branch\n",
    "        cp = cp[:end_index]\n",
    "        mass = mass[:end_index] \n",
    "        radius = radius[:end_index]\n",
    "\n",
    "        # Apply Minimum Mass Cutoff (Filter low-mass instability)\n",
    "        M_CUTOFF = 0.15 # Filter out anything below 0.15 solar masses\n",
    "        low_mass_mask = mass >= M_CUTOFF\n",
    "        cp= cp[low_mass_mask]\n",
    "        mass = mass[low_mass_mask]\n",
    "        radius = radius[low_mass_mask]\n",
    "\n",
    "        # Basic filtering\n",
    "        valid_mask = (radius > 0) & (mass > 0)\n",
    "        cp = cp[valid_mask]\n",
    "        mass = mass[valid_mask]\n",
    "        radius = radius[valid_mask]\n",
    "\n",
    "        if len(mass) == 0: continue\n",
    "\n",
    "        # Create feature vector for every point\n",
    "        # Features: [m, L, J, n_v, d, B, n, cp] -> Predict: Mass, Radius\n",
    "        num_points = len(mass)\n",
    "        eos_params = np.array([m, L, J, n_v, d, B, n])\n",
    "        \n",
    "        # Tile EOS params to match number of mass points\n",
    "        eos_repeated = np.tile(eos_params, (num_points, 1))\n",
    "\n",
    "        # Stack: [EOS_Params (7), Central Pressure (1), Mass (1), Radius (1)]\n",
    "        rows = np.column_stack([eos_repeated, cp, mass, radius])\n",
    "        dataset_rows.append(rows)\n",
    "\n",
    "    if not dataset_rows: return None\n",
    "    return np.vstack(dataset_rows), np.array(files_used_for_training)\n",
    "\n",
    "print(\"Building Training Set...\")\n",
    "train_data = process_files(train_files, \"Train\")\n",
    "print(\"Building Validation Set...\")\n",
    "val_data = process_files(val_files, \"Val\")\n",
    "print(\"Datasets created!\")\n",
    "\n",
    "print(train_data[1].shape)\n",
    "print(val_data[1].shape)\n",
    "\n",
    "files_used_for_training = np.concatenate([train_data[1], val_data[1]]) if train_data and val_data is not None else []\n",
    "\n",
    "save_dir = f\"{NUM_FILES_TO_USE}files\"\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_dir, \"train_data_files.npy\"), train_data[0])\n",
    "np.save(os.path.join(save_dir, \"val_data_files.npy\"), val_data[0])\n",
    "np.save(os.path.join(save_dir, \"files_used_for_training.npy\"), files_used_for_training)\n",
    "\n",
    "print(\"Datasets saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05456348",
   "metadata": {},
   "source": [
    "# <div style= 'color: red'> FROM HERE ON USE CLUSTER FILES train_real.py AND test.py, FOR BIGGER RUNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53906f3",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Loading Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa660cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"1000files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(save_dir, \"train_data_files.npy\"))\n",
    "val_data = np.load(os.path.join(save_dir, \"val_data_files.npy\"))\n",
    "files_used_for_training = np.load(os.path.join(save_dir, \"files_used_for_training.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78087437",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a72fd",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Range of values of the parameters from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nuclear Mass min/max:\", np.round(train_data[:, 0].min(), 3), np.round(train_data[:, 0].max(), 3))\n",
    "print(\"Slope parmeter min/max:\", np.round(train_data[:, 1].min(), 3), np.round(train_data[:, 1].max(), 3))\n",
    "print(\"Symmetry Energy min/max:\", np.round(train_data[:, 2].min(), 3), np.round(train_data[:, 2].max(), 3))\n",
    "print(\"Vector coupling min/max:\", np.round(train_data[:, 3].min(), 3), np.round(train_data[:, 3].max(), 3))\n",
    "print(\"Dipole coupling min/max:\", np.round(train_data[:, 4].min(), 3), np.round(train_data[:, 4].max(), 3))\n",
    "print(\"Bag constant min/max:\", np.round(train_data[:, 5].min(), 3), np.round(train_data[:, 5].max(), 3))\n",
    "print(\"Number of particles min/max:\", np.round(train_data[:, 6].min(), 3), np.round(train_data[:, 6].max(), 3))\n",
    "\n",
    "print(\"Central Pressure min/max:\", np.round(train_data[:, 7].min(), 3), np.round(train_data[:, 7].max(), 3))\n",
    "print(\"Mass min/max:\", np.round(train_data[:, 8].min(), 3), np.round(train_data[:, 8].max(), 3))\n",
    "print(\"Radius min/max:\", np.round(train_data[:, 9].min(), 3), np.round(train_data[:, 9].max(), 3))\n",
    "\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c2967",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Produce a mass-radius curve for a single EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f701c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Extract ONE eos curve\n",
    "# -------------------------------------------------------\n",
    "# Pick a specific index (0 = first file in train_data before shuffling)\n",
    "# But because the train_data is shuffled, we instead extract a *single EOS*\n",
    "# by matching identical (m, L, J, n_v, d, B, n).\n",
    "# Selecting the EOS at index 20000 for variety\n",
    "m0, L0, J0, nv0, d0, B0, n0 = train_data[20000, :7]\n",
    "\n",
    "mask = (\n",
    "    (train_data[:,0] == m0)  &\n",
    "    (train_data[:,1] == L0)  &\n",
    "    (train_data[:,2] == J0)  &\n",
    "    (train_data[:,3] == nv0) &\n",
    "    (train_data[:,4] == d0)  &\n",
    "    (train_data[:,5] == B0)  &\n",
    "    (train_data[:,6] == n0)\n",
    ")\n",
    "\n",
    "subset = train_data[mask]\n",
    "\n",
    "mass = subset[:, 8]\n",
    "radius = subset[:, 9]\n",
    "\n",
    "# Sort by mass so plot looks clean\n",
    "order = np.argsort(mass)\n",
    "mass = mass[order]\n",
    "radius = radius[order]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Plot\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(radius, mass, linewidth=2)\n",
    "plt.xlabel(\"Radius [km]\")\n",
    "plt.ylabel(\"Mass [M☉]\")\n",
    "plt.title(f\"EOS sanity check\\nm={m0}, L={L0}, J={J0}, n_v={nv0}, d={d0}, B={B0}, n={n0}\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892617e5",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Data processing pipeline\n",
    "\n",
    "* Z_Score normalization: $X_{\\text{norm}, i} = (X_i - \\mu_i) / \\sigma_i$\n",
    "* Constant Scaling: $X_{\\text{norm}, M} = X_M / 3.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor\n",
    "# Inputs: Cols 0 to 7 (8 features: m, L, J, n_v, d, B, n, cp)\n",
    "# Output: Col 8 and 9 (Mass, Radius)\n",
    "X_train = torch.tensor(train_data[:, :8], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data[:, 8:], dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(val_data[:, :8], dtype=torch.float32)\n",
    "y_val = torch.tensor(val_data[:, 8:], dtype=torch.float32)\n",
    "\n",
    "# 1. Separate Central Pressure (index 7) from EOS parameters (indices 0-7)\n",
    "X_eos_train, X_cp_train = X_train[:, :7], X_train[:, 7:]\n",
    "X_eos_val, X_cp_val = X_val[:, :7], X_val[:, 7:]\n",
    "\n",
    "# 2.1 Normalize using log scaling on Central Pressure (cp)\n",
    "X_cp_train_norm = torch.log10(X_cp_train)\n",
    "X_cp_val_norm = torch.log10(X_cp_val)\n",
    "\n",
    "# 2.2 Normalize using Z-Score on EOS Parameters (m, L, J, n_v, d, B, n)\n",
    "X_eos_mean = X_eos_train.mean(dim=0, keepdim=True)\n",
    "X_eos_std = X_eos_train.std(dim=0, keepdim=True)\n",
    "X_eos_std[X_eos_std == 0] = 1.0 \n",
    "\n",
    "# Save them:\n",
    "torch.save(X_eos_mean, os.path.join(save_dir, \"X_eos_mean.pt\"))\n",
    "torch.save(X_eos_std, os.path.join(save_dir, \"X_eos_std.pt\"))\n",
    "print(\"Normalization statistics saved.\")\n",
    "\n",
    "X_eos_train_norm = (X_eos_train - X_eos_mean) / X_eos_std\n",
    "X_eos_val_norm = (X_eos_val - X_eos_mean) / X_eos_std\n",
    "\n",
    "# 3. --- Recombine inputs ---\n",
    "X_train_norm = torch.cat((X_eos_train_norm, X_cp_train_norm), dim=1)\n",
    "X_val_norm = torch.cat((X_eos_val_norm, X_cp_val_norm), dim=1)\n",
    "\n",
    "\n",
    "# 4. Separate Mass and Radius\n",
    "y_mass_train, y_radius_train = y_train[:, 0:1], y_train[:, 1:2]\n",
    "y_mass_val, y_radius_val = y_val[:, 0:1], y_val[:, 1:2]\n",
    "\n",
    "# 5.1 Normalize using Constant Scaling on Mass (M)\n",
    "MASS_SCALE = 3.5 \n",
    "y_mass_train_norm = y_mass_train / MASS_SCALE\n",
    "y_mass_val_norm = y_mass_val / MASS_SCALE\n",
    "\n",
    "# 5.2. Normalize using Constant Scaling on Radius (R)\n",
    "RADIUS_SCALE = 25.0\n",
    "y_radius_train_norm = y_radius_train / RADIUS_SCALE\n",
    "y_radius_val_norm = y_radius_val / RADIUS_SCALE\n",
    "\n",
    "# 6. --- Recombine Outputs ---\n",
    "y_train_norm = torch.cat((y_mass_train_norm, y_radius_train_norm), dim=1)\n",
    "y_val_norm = torch.cat((y_mass_val_norm, y_radius_val_norm), dim=1)\n",
    "\n",
    "# Note: You no longer need y_mean and y_std for normalization, \n",
    "# but you MUST save RADIUS_SCALE, MASS_SCALE and inverse log for cp to de-normalize predictions later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cb7d0",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Range of values of the parameters after scaling and normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nuclear Mass min/max:\", np.round(X_train_norm[:, 0].min(), 3), np.round(X_train_norm[:, 0].max(), 3))\n",
    "print(\"Slope parmeter min/max:\", np.round(X_train_norm[:, 1].min(), 3), np.round(X_train_norm[:, 1].max(), 3))\n",
    "print(\"Symmetry Energy min/max:\", np.round(X_train_norm[:, 2].min(), 3), np.round(X_train_norm[:, 2].max(), 3))\n",
    "print(\"Vector coupling min/max:\", np.round(X_train_norm[:, 3].min(), 3), np.round(X_train_norm[:, 3].max(), 3))\n",
    "print(\"Dipole coupling min/max:\", np.round(X_train_norm[:, 4].min(), 3), np.round(X_train_norm[:, 4].max(), 3))\n",
    "print(\"Bag constant min/max:\", np.round(X_train_norm[:, 5].min(), 3), np.round(X_train_norm[:, 5].max(), 3))\n",
    "print(\"Number of particles min/max:\", np.round(X_train_norm[:, 6].min(), 3), np.round(X_train_norm[:, 6].max(), 3))\n",
    "print(\"Central Pressure min/max:\", np.round(X_train_norm[:, 7].min(), 3), np.round(X_train_norm[:, 7].max(), 3))\n",
    "\n",
    "print(\"Mass min/max:\", np.round(y_train_norm[:, 0].min(), 3), np.round(y_train_norm[:, 0].max(), 3))\n",
    "print(\"Radius min/max:\", np.round(y_train_norm[:, 1].min(), 3), np.round(y_train_norm[:, 1].max(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd70e28",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, auxiliary_dim=1):\n",
    "        super().__init__()\n",
    "        # We accept the hidden state + the auxiliary Central Pressure injection\n",
    "        self.fc = nn.Linear(hidden_dim + auxiliary_dim, hidden_dim)\n",
    "        self.act = nn.GELU() # Gaussian Error Linear Unit\n",
    "    \n",
    "    def forward(self, x, cp):\n",
    "        # Concatenate Central Pressure to the input of the layer\n",
    "        combined = torch.cat([x, cp], dim=1)\n",
    "        out = self.act(self.fc(combined))\n",
    "        return x + out # Residual connection\n",
    "\n",
    "class PhysicsEmulator(nn.Module):\n",
    "    def __init__(self, input_dim=8, hidden_dim=512): \n",
    "        super().__init__()\n",
    "        # Separate EOS inputs from Central Pressure\n",
    "        # input_dim is 8: (7 EOS params + 1 Central Pressure)\n",
    "        self.eos_dim = input_dim - 1 \n",
    "        \n",
    "        # 1. Initial encoding of EOS parameters only: \n",
    "        self.input_layer = nn.Linear(self.eos_dim, hidden_dim)\n",
    "        \n",
    "        # 2. Deep Residual Layers with Central Pressure Injection\n",
    "        self.block1 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "        self.block2 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "        self.block3 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "        self.block4 = ResNetBlock(hidden_dim, auxiliary_dim=1)\n",
    "\n",
    "        # 3. Output layers\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 1, hidden_dim // 2), # Inject Central Pressure one last time\n",
    "            nn.GELU(),                                      # 513 --> 256\n",
    "            nn.Linear(hidden_dim // 2, 2)                   # Output: 256 --> 2\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into EOS params and Central Pressure\n",
    "        # Central Pressure is the LAST column (index -1)\n",
    "        eos_params = x[:, :-1]\n",
    "        cp = x[:, -1:]\n",
    "        \n",
    "        # 1. Encode EOS\n",
    "        x_hidden = self.input_layer(eos_params)             # Input: 7 --> 512\n",
    "\n",
    "        # 2. Central Pressure-Injected Residual Flow: Pass through blocks, injecting Central Pressure at each step\n",
    "        x_hidden = self.block1(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        x_hidden = self.block2(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        x_hidden = self.block3(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        x_hidden = self.block4(x_hidden, cp)              # 512 + 1 --> 512\n",
    "        \n",
    "        # 3. Final Prediction\n",
    "        # Concatenate Central Pressure one last time for the read-out\n",
    "        combined_final = torch.cat([x_hidden, cp], dim=1) # 512 + 1 = 513\n",
    "        return self.final_layer(combined_final)             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50670815",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function for plotting ---\n",
    "def plot_and_save_losses(train_losses, val_losses, filename=\"loss_curve.png\"):\n",
    "    \"\"\"Plots training and validation loss and saves the figure.\"\"\"\n",
    "    epochs = range(len(train_losses))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss', color='red')\n",
    "    \n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss (Normalized)')\n",
    "    plt.yscale('log') # Use log scale for clearer visualization of small losses\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    \n",
    "    try:\n",
    "        plt.savefig(os.path.join(save_dir, filename))\n",
    "        print(f\"Loss plot saved to {filename}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving plot: {e}\", flush=True)\n",
    "    plt.close()\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "model = PhysicsEmulator().to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-7)\n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAINING\n",
    "# ==============================================================================\n",
    "# Ensure Mass and Radius are Torch Tensors (it might be a numpy array currently)\n",
    "if isinstance(y_train_norm, np.ndarray):\n",
    "    y_train_norm = torch.from_numpy(y_train_norm).to(torch.float32)\n",
    "if isinstance(y_val_norm, np.ndarray):\n",
    "    y_val_norm = torch.from_numpy(y_val_norm).to(torch.float32)\n",
    "\n",
    "# Update your DataLoaders\n",
    "train_loader = DataLoader(TensorDataset(X_train_norm, y_train_norm), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_norm, y_val_norm), batch_size=BATCH_SIZE, shuffle=False)\n",
    "epochs = 100 \n",
    "best_loss = float('inf')\n",
    "\n",
    "# 1. ADD STORAGE LISTS\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Ensure MASS_SCALE and RADIUS_SCALE are defined globally or passed in if running as a function\n",
    "# If you didn't define save_dir, model will save in current directory.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_b, y_b in train_loader:\n",
    "        X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_b)\n",
    "        loss = criterion(pred, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader:\n",
    "            X_b, y_b = X_b.to(DEVICE), y_b.to(DEVICE)\n",
    "            pred = model(X_b)\n",
    "            loss = criterion(pred, y_b)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 2. APPEND LOSSES\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        # Removed os.path.join(save_dir, \"Best_EOS_Model.pth\") for simplicity, assuming current directory\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"Best_EOS_Model.pth\"))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # Calculate the Approximate Physical Error in km\n",
    "        phys_error_km = np.sqrt(2 * val_loss) * RADIUS_SCALE \n",
    "        mass_error = np.sqrt(2 * val_loss) * MASS_SCALE\n",
    "\n",
    "        print(f\"Epoch {epoch} | Val Loss: {val_loss:.6e} | Approx Error: {phys_error_km:.4f} km | Approx Mass Error: {mass_error:.4f}\", flush=True)\n",
    "\n",
    "        # 3. PLOT AND SAVE PERIODICALLY\n",
    "        # Plot every 50 epochs (or choose a different interval)\n",
    "        if epoch % 50 == 0 and epoch > 0:\n",
    "            plot_and_save_losses(train_losses, val_losses, filename=f\"loss_curve_epoch{epoch}.png\")\n",
    "\n",
    "# 4. FINAL PLOT after training finishes\n",
    "plot_and_save_losses(train_losses, val_losses, filename=\"loss_curve_final.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a45c3",
   "metadata": {},
   "source": [
    "### <div style= 'color: yellow'> Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da01a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & HELPERS\n",
    "# ==============================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RADIUS_SCALE = 25.0\n",
    "MASS_SCALE = 3.5\n",
    "\n",
    "# Regex must match the Training Pipeline exactly\n",
    "pattern = re.compile(\n",
    "    r\"MREoSm(?P<m>\\d+)\"\n",
    "    r\"L(?P<L>\\d+)\"\n",
    "    r\"J(?P<J>\\d+)\"\n",
    "    r\"T(?P<T>\\d+)_\"\n",
    "    r\"n(?P<n>\\d+)_\"\n",
    "    r\"Yp?\\+.*?RGgen_\"\n",
    "    r\"v(?P<n_v>[-\\d\\.]+)\"\n",
    "    r\"d(?P<d>[-\\d\\.]+)\"\n",
    "    r\"B(?P<Btype>[np])(?P<B>\\d+)\\.dat\"\n",
    ")\n",
    "\n",
    "def extract_eos_params(filename):\n",
    "    match = pattern.match(filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    # Extract raw values exactly like training\n",
    "    m = float(match.group(\"m\")) / 100.0\n",
    "    L = float(match.group(\"L\"))\n",
    "    J = float(match.group(\"J\"))\n",
    "    n_v = float(match.group(\"n_v\"))\n",
    "    d = float(match.group(\"d\"))\n",
    "    B = float(match.group(\"B\")) / 1000.0\n",
    "    n = float(match.group('n')) / 1000.0  # e.g., 0.160\n",
    "    \n",
    "    # Return as a numpy array for easy stacking [m, L, J, n_v, d, B, n]\n",
    "    return np.array([m, L, J, n_v, d, B, n], dtype=np.float32)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. SELECT \"UNSEEN\" FILES\n",
    "# ==============================================================================\n",
    "# Path to your data\n",
    "DATA_DIR_PATH = \"/home/kay/ML-for-Neutron-Stars/3_PP_EOS_emulator_real_data/All_MR_Relations/All_MR_Relations/\"\n",
    "all_files = glob.glob(os.path.join(DATA_DIR_PATH, \"MR*.dat\"))\n",
    "print(\"Total files available:\", len(all_files))\n",
    "\n",
    "# Load the list of files used for training\n",
    "# Ensure this file exists from your training run!\n",
    "try:\n",
    "    training_files_used = np.load(os.path.join(save_dir, \"files_used_for_training.npy\"), allow_pickle=True)\n",
    "    # Convert to set for fast lookup\n",
    "    training_set = {\n",
    "    os.path.basename(f) for f in training_files_used        \n",
    "    }\n",
    "    print(\"Length of training set:\", len(training_set))\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: List of training files not found. Using random files (risk of data leakage).\")\n",
    "    # training_set = set()\n",
    "    training_set = {\n",
    "    os.path.basename(f) for f in training_files_used        \n",
    "    }\n",
    "    print(\"Length of random training set:\", len(training_set))\n",
    "\n",
    "# Filter: Keep only files NOT in the training set\n",
    "test_pool = [\n",
    "    f for f in all_files\n",
    "    if os.path.basename(f) not in training_set\n",
    "]\n",
    "print(\"Total unseen files available for testing:\", len(test_pool))\n",
    "\n",
    "# Ensure we have enough unseen files\n",
    "if len(test_pool) < 5:\n",
    "    print(\"Warning: Not enough test files found. Check your paths.\")\n",
    "    selected_files = all_files[:5]\n",
    "else:\n",
    "    # Pick 10 random UNSEEN files <<--------------------------------------------\n",
    "    unseen_files = 10\n",
    "    selected_files = np.random.choice(test_pool, size=unseen_files, replace=False)\n",
    "print(f\"Testing on {len(selected_files)} unseen files.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PREPARE MODEL & SCALERS\n",
    "# ==============================================================================\n",
    "# Load Model\n",
    "model = PhysicsEmulator(input_dim=8).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"Best_EOS_Model.pth\"), map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# IMPORTANT: You must use the SAME mean/std from TRAINING.\n",
    "# If you are in a new session, you should have saved these. \n",
    "# For now, assuming they exist in memory or you manually input them.\n",
    "# Example: \n",
    "# X_eos_mean = torch.load(\"Datasets/X_eos_mean.pt\")\n",
    "# X_eos_std = torch.load(\"Datasets/X_eos_std.pt\")\n",
    "\n",
    "if 'X_eos_mean' not in locals():\n",
    "    # Loading the SAME mean/std from TRAINING.\n",
    "    X_eos_mean = torch.load(os.path.join(save_dir, \"X_eos_mean.pt\"), map_location=DEVICE)\n",
    "    X_eos_std = torch.load(os.path.join(save_dir, \"X_eos_std.pt\"), map_location=DEVICE)\n",
    "    print(\"Using saved normalization statistics.\")\n",
    "else:\n",
    "    print(\"Using existing normalization statistics in memory.\")\n",
    "    X_eos_mean = X_eos_mean.to(DEVICE)\n",
    "    X_eos_std = X_eos_std.to(DEVICE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EVALUATION LOOP\n",
    "# ==============================================================================\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(selected_files)))\n",
    "\n",
    "for color, file_path in zip(colors, selected_files):\n",
    "    filename = os.path.basename(file_path)\n",
    "    eos_params = extract_eos_params(filename)\n",
    "    \n",
    "    if eos_params is None: continue\n",
    "\n",
    "    # Load Ground Truth Data\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "    except: continue\n",
    "        \n",
    "    # get cp, mass and radius values    \n",
    "    cp_vals = data[:, 0]\n",
    "    mass_vals = data[:, 1]\n",
    "    radius_vals = data[:, 2]\n",
    "    \n",
    "    # Filter stable branch (Up to Max Mass) same as training\n",
    "    max_m_idx = np.argmax(mass_vals)\n",
    "    # Apply the same safety margin cut as training\n",
    "    cut_idx = max(1, max_m_idx - 2) \n",
    "    \n",
    "    cp_vals = cp_vals[:cut_idx]\n",
    "    mass_vals = mass_vals[:cut_idx]\n",
    "    radius_vals = radius_vals[:cut_idx]\n",
    "\n",
    "    # Basic filtering to ensure positive values\n",
    "    valid_mask = (radius_vals > 0) & (mass_vals > 0)\n",
    "    cp_vals     = cp_vals[valid_mask]\n",
    "    mass_vals   = mass_vals[valid_mask]\n",
    "    radius_vals = radius_vals[valid_mask]\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # PREPARE INPUT TENSORS (The Hybrid Normalization)\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Tile the EOS params for every mass point\n",
    "    num_points = len(mass_vals)\n",
    "    eos_repeated = np.tile(eos_params, (num_points, 1)) # Shape (N, 7)\n",
    "    \n",
    "    # 2. Convert to Tensor\n",
    "    X_eos_input = torch.tensor(eos_repeated, dtype=torch.float32).to(DEVICE)\n",
    "    X_cp_input = torch.tensor(cp_vals.reshape(-1, 1), dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    # 3. NORMALIZE (Use training stats!)\n",
    "    # EOS: Z-score\n",
    "    X_eos_norm = (X_eos_input - X_eos_mean) / X_eos_std\n",
    "    # CP: Log Scaling\n",
    "    X_cp_norm = torch.log10(X_cp_input)\n",
    "    \n",
    "    # 4. Concatenate\n",
    "    model_input = torch.cat((X_eos_norm, X_cp_norm), dim=1)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # PREDICT\n",
    "    # ---------------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        # Predict normalized Mass and Radius (0.0 to ~1.0)\n",
    "        pred_norm = model(model_input)\n",
    "        \n",
    "        # De-normalize: M_phys = M_norm * 3.5\n",
    "        M_pred_solar = pred_norm[:, 0] * MASS_SCALE\n",
    "        M_pred_solar = M_pred_solar.cpu().numpy()\n",
    "        \n",
    "        # De-normalize: R_phys = R_norm * 25.0\n",
    "        R_pred_km = pred_norm[:, 1] * RADIUS_SCALE\n",
    "        R_pred_km = R_pred_km.cpu().numpy()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # PLOT [m, L, J, n_v, d, B, n] # Label with Physics Params\n",
    "    # ---------------------------------------------------------\n",
    "    label_txt = f\"\"\"m={eos_params[0]:.2f}, L={eos_params[1]:.0f}, J={eos_params[2]:.0f}, n_v={eos_params[3]:.2f}, d={eos_params[4]:.2f}, B={eos_params[5]:.2f}, n={eos_params[6]:.3f}\"\"\" \n",
    "    \n",
    "    # Plot Ground Truth (Solid Line)\n",
    "    plt.plot(radius_vals, mass_vals, \"-\", color=color, alpha=0.5, linewidth=2)\n",
    "    \n",
    "    # Plot Prediction (Dashed Line) \n",
    "    plt.plot(R_pred_km, M_pred_solar, \"--\", color=color, linewidth=2, label=label_txt)\n",
    "\n",
    "plt.title(\"Neural Network vs. TOV Solver (Unseen Files)\")\n",
    "plt.xlabel(\"Radius (km)\")\n",
    "plt.ylabel(r\"Mass ($M_{\\odot}$)\")\n",
    "plt.legend(title=\"Predictions\", fontsize='small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(9, 20) # Focus on typical NS radius range\n",
    "plt.ylim(0, 3.5)\n",
    "# plt.savefig(os.path.join(save_dir, f\"Testing plot {unseen_files}files.png\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd7d50",
   "metadata": {},
   "source": [
    "### <div style= 'color: orange'> Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be32bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "orig_radius_all = radius_vals\n",
    "pred_radius_all = R_pred_km\n",
    "\n",
    "orig_mass_all = mass_vals\n",
    "pred_mass_all = M_pred_solar\n",
    "\n",
    "mae_radius = mean_absolute_error(orig_radius_all, pred_radius_all)\n",
    "rmse_radius = np.sqrt(mean_squared_error(orig_radius_all, pred_radius_all))\n",
    "r2_radius = r2_score(orig_radius_all, pred_radius_all)\n",
    "\n",
    "mse_mass = mean_squared_error(orig_mass_all, pred_mass_all)\n",
    "rmse_mass = np.sqrt(mse_mass)\n",
    "r2_mass = r2_score(orig_mass_all, pred_mass_all)\n",
    "\n",
    "norm_rmse = np.mean([\n",
    "    rmse_radius / np.mean(orig_radius_all),\n",
    "    rmse_mass / np.mean(orig_mass_all)\n",
    "])\n",
    "\n",
    "print(f\"Radius:  MAE={mae_radius:.4f}, RMSE={rmse_radius:.4f}, R²={r2_radius:.4f}\")\n",
    "print(f\"Mass:  RMSE={rmse_mass:.4f}, R²={r2_mass:.4f}\")\n",
    "print(f\"Combined normalized RMSE = {norm_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Save metrics to a text file\n",
    "# -----------------------------\n",
    "output_file = os.path.join(save_dir, \"Model_metrics.txt\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Radius and TD prediction metrics\\n\")\n",
    "    f.write(\"================================\\n\")\n",
    "    f.write(f\"MAE   : {mae_radius:.6f}\\n\")\n",
    "    f.write(f\"RMSE  : {rmse_radius:.6f}\\n\")\n",
    "    f.write(f\"R^2   : {r2_radius:.6f}\\n\")\n",
    "    f.write(f\"Norm RMSE : {norm_rmse:.6f}\\n\")\n",
    "\n",
    "print(f\"Metrics saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fedc96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
